# -*- coding: utf-8 -*-
"""SystemyAI_lab_7_2_Gradient.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qATP0ZLkmFHtoEBBhHY6uxq6dmLnWobX
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

import keras
from keras.models import Sequential
from keras.layers import Dense

"""Zbiór danych:"""

x_label1 = np.random.normal(3, 1, 1000)
y_label1 = np.random.normal(2, 1, 1000)
x_label2 = np.random.normal(7, 1, 1000)
y_label2 = np.random.normal(6, 1, 1000)

xs = np.append(x_label1, x_label2)
ys = np.append(y_label1, y_label2)
labels = np.asarray([0.]*len(x_label1)+[1.]*len(x_label2))

plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.show()

"""Funkcja błędu (entropia krzyżowa):"""

def loss_fn(label, label_model):
    return tf.reduce_mean(-label*tf.math.log(label_model)-(1-label)*tf.math.log(1-label_model))

"""Początkowe wartości parametrów i pętla ucząca:"""

import random
a = tf.Variable(random.random())
b = tf.Variable(random.random())

c = tf.Variable(random.random())

Loss = []
epochs = 3000
lr = 0.1

for _ in range(epochs):

  with tf.GradientTape() as tape:
    #predykcja modelu

    pred_y = tf.sigmoid(a * xs + b * ys + c)
    #policzenie błędu
    loss = loss_fn(labels, pred_y)
    #zapisanie aktualnej wartości błędu
    Loss.append(loss.numpy())

  dloss_da, dloss_db, dloss_dc = tape.gradient(loss,(a, b,c))

  a.assign_sub(lr*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(lr*dloss_db)  #b = b - alpha*dloss_db
  c.assign_sub(lr*dloss_dc)

Loss

plt.plot(Loss)
plt.show()

"""Sprawdzamy dla pewnego punktu:"""

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter([x],[y],c='b', marker='s')
plt.show()
#print(a,b,c)
tf.sigmoid(a*x + b*y + c).numpy()

tf.sigmoid(a*x + b*y + c).numpy()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

x=5.0
y=4.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

"""##Ilość epok 2000"""



import random
a = tf.Variable(random.random())
b = tf.Variable(random.random())

c = tf.Variable(random.random())

Loss = []
epochs = 2000
lr = 0.1

for _ in range(epochs):

  with tf.GradientTape() as tape:
    #predykcja modelu

    pred_y = tf.sigmoid(a * xs + b * ys + c)
    #policzenie błędu
    loss = loss_fn(labels, pred_y)
    #zapisanie aktualnej wartości błędu
    Loss.append(loss.numpy())

  dloss_da, dloss_db, dloss_dc = tape.gradient(loss,(a, b,c))

  a.assign_sub(lr*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(lr*dloss_db)  #b = b - alpha*dloss_db
  c.assign_sub(lr*dloss_dc)

Loss

plt.plot(Loss)
plt.show()

"""Sprawdzamy dla pewnego punktu:"""

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter([x],[y],c='b', marker='s')
plt.show()
#print(a,b,c)
tf.sigmoid(a*x + b*y + c).numpy()

tf.sigmoid(a*x + b*y + c).numpy()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

x=5.0
y=4.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

"""#Ilosc epok 100"""

import random
a = tf.Variable(random.random())
b = tf.Variable(random.random())

c = tf.Variable(random.random())

Loss = []
epochs = 100
lr = 0.1

for _ in range(epochs):

  with tf.GradientTape() as tape:
    #predykcja modelu

    pred_y = tf.sigmoid(a * xs + b * ys + c)
    #policzenie błędu
    loss = loss_fn(labels, pred_y)
    #zapisanie aktualnej wartości błędu
    Loss.append(loss.numpy())

  dloss_da, dloss_db, dloss_dc = tape.gradient(loss,(a, b,c))

  a.assign_sub(lr*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(lr*dloss_db)  #b = b - alpha*dloss_db
  c.assign_sub(lr*dloss_dc)

Loss

plt.plot(Loss)
plt.show()

"""Sprawdzamy dla pewnego punktu:"""

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter([x],[y],c='b', marker='s')
plt.show()
#print(a,b,c)
tf.sigmoid(a*x + b*y + c).numpy()

tf.sigmoid(a*x + b*y + c).numpy()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

x=5.0
y=4.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

"""#Ilosc epok - 6000"""



import random
a = tf.Variable(random.random())
b = tf.Variable(random.random())

c = tf.Variable(random.random())

Loss = []
epochs = 6000
lr = 0.1

for _ in range(epochs):

  with tf.GradientTape() as tape:
    #predykcja modelu

    pred_y = tf.sigmoid(a * xs + b * ys + c)
    #policzenie błędu
    loss = loss_fn(labels, pred_y)
    #zapisanie aktualnej wartości błędu
    Loss.append(loss.numpy())

  dloss_da, dloss_db, dloss_dc = tape.gradient(loss,(a, b,c))

  a.assign_sub(lr*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(lr*dloss_db)  #b = b - alpha*dloss_db
  c.assign_sub(lr*dloss_dc)

Loss

plt.plot(Loss)
plt.show()

"""Sprawdzamy dla pewnego punktu:"""

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter([x],[y],c='b', marker='s')
plt.show()
#print(a,b,c)
tf.sigmoid(a*x + b*y + c).numpy()

tf.sigmoid(a*x + b*y + c).numpy()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

x=5.0
y=4.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

"""#Learning rate - 0.005"""



import random
a = tf.Variable(random.random())
b = tf.Variable(random.random())

c = tf.Variable(random.random())

Loss = []
epochs = 3000
lr = 0.005

for _ in range(epochs):

  with tf.GradientTape() as tape:
    #predykcja modelu

    pred_y = tf.sigmoid(a * xs + b * ys + c)
    #policzenie błędu
    loss = loss_fn(labels, pred_y)
    #zapisanie aktualnej wartości błędu
    Loss.append(loss.numpy())

  dloss_da, dloss_db, dloss_dc = tape.gradient(loss,(a, b,c))

  a.assign_sub(lr*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(lr*dloss_db)  #b = b - alpha*dloss_db
  c.assign_sub(lr*dloss_dc)

Loss

plt.plot(Loss)
plt.show()

"""Sprawdzamy dla pewnego punktu:"""

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter([x],[y],c='b', marker='s')
plt.show()
#print(a,b,c)
tf.sigmoid(a*x + b*y + c).numpy()

tf.sigmoid(a*x + b*y + c).numpy()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

x=5.0
y=4.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

"""#Learning rate - 0.5"""



import random
a = tf.Variable(random.random())
b = tf.Variable(random.random())

c = tf.Variable(random.random())

Loss = []
epochs = 3000
lr = 0.5

for _ in range(epochs):

  with tf.GradientTape() as tape:
    #predykcja modelu

    pred_y = tf.sigmoid(a * xs + b * ys + c)
    #policzenie błędu
    loss = loss_fn(labels, pred_y)
    #zapisanie aktualnej wartości błędu
    Loss.append(loss.numpy())

  dloss_da, dloss_db, dloss_dc = tape.gradient(loss,(a, b,c))

  a.assign_sub(lr*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(lr*dloss_db)  #b = b - alpha*dloss_db
  c.assign_sub(lr*dloss_dc)

Loss

plt.plot(Loss)
plt.show()

"""Sprawdzamy dla pewnego punktu:"""

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter([x],[y],c='b', marker='s')
plt.show()
#print(a,b,c)
tf.sigmoid(a*x + b*y + c).numpy()

tf.sigmoid(a*x + b*y + c).numpy()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

x=5.0
y=4.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

"""#Mini batch"""



def subset_dataset_2(x_dataset, y_dataset,label,subset_size):
    arr = np.arange(len(x_dataset))
    np.random.shuffle(arr)
    x_train = x_dataset[arr[0:subset_size]]
    y_train = y_dataset[arr[0:subset_size]]
    label_train = label[arr[0:subset_size]]
    return x_train,y_train,label_train

Loss = []
epochs = 1000
learning_rate = 0.01
batch_size = 50
a = tf.Variable(random.random())
b = tf.Variable(random.random())
c = tf.Variable(random.random())
for _ in range(epochs):
  xs_batch,ys_batch,labels_batch = subset_dataset_2(xs,ys,labels,batch_size)
  with tf.GradientTape() as tape:
    pred_l = tf.sigmoid(a * xs_batch + b * ys_batch + c)
    #print(label_batch.shape)
    loss = loss_fn(labels_batch, pred_l)
    Loss.append(loss.numpy())

  dloss_da, dloss_db, dloss_dc = tape.gradient(loss,(a, b,c))

  a.assign_sub(learning_rate*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(learning_rate*dloss_db)  #b = b - alpha*dloss_db
  c.assign_sub(learning_rate*dloss_dc)

plt.plot(Loss)
plt.show()

"""Mini batch - 100"""

Loss = []
epochs = 1000
learning_rate = 0.01
batch_size = 100
a = tf.Variable(random.random())
b = tf.Variable(random.random())
c = tf.Variable(random.random())
for _ in range(epochs):
  xs_batch,ys_batch,labels_batch = subset_dataset_2(xs,ys,labels,batch_size)
  with tf.GradientTape() as tape:
    pred_l = tf.sigmoid(a * xs_batch + b * ys_batch + c)
    #print(label_batch.shape)
    loss = loss_fn(labels_batch, pred_l)
    Loss.append(loss.numpy())

  dloss_da, dloss_db, dloss_dc = tape.gradient(loss,(a, b,c))

  a.assign_sub(learning_rate*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(learning_rate*dloss_db)  #b = b - alpha*dloss_db
  c.assign_sub(learning_rate*dloss_dc)

Loss

plt.plot(Loss)
plt.show()

"""Sprawdzamy dla pewnego punktu:"""

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter([x],[y],c='b', marker='s')
plt.show()
#print(a,b,c)
tf.sigmoid(a*x + b*y + c).numpy()

tf.sigmoid(a*x + b*y + c).numpy()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

x=5.0
y=4.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

"""Mini batch - 500"""



Loss = []
epochs = 1000
learning_rate = 0.01
batch_size = 500
a = tf.Variable(random.random())
b = tf.Variable(random.random())
c = tf.Variable(random.random())
for _ in range(epochs):
  xs_batch,ys_batch,labels_batch = subset_dataset_2(xs,ys,labels,batch_size)
  with tf.GradientTape() as tape:
    pred_l = tf.sigmoid(a * xs_batch + b * ys_batch + c)
    #print(label_batch.shape)
    loss = loss_fn(labels_batch, pred_l)
    Loss.append(loss.numpy())

  dloss_da, dloss_db, dloss_dc = tape.gradient(loss,(a, b,c))

  a.assign_sub(learning_rate*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(learning_rate*dloss_db)  #b = b - alpha*dloss_db
  c.assign_sub(learning_rate*dloss_dc)

Loss

plt.plot(Loss)
plt.show()

"""Sprawdzamy dla pewnego punktu:"""

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter([x],[y],c='b', marker='s')
plt.show()
#print(a,b,c)
tf.sigmoid(a*x + b*y + c).numpy()

tf.sigmoid(a*x + b*y + c).numpy()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

x=5.0
y=4.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

"""Mini batch - 1000"""

Loss = []
epochs = 1000
learning_rate = 0.01
batch_size = 1000
a = tf.Variable(random.random())
b = tf.Variable(random.random())
c = tf.Variable(random.random())
for _ in range(epochs):
  xs_batch,ys_batch,labels_batch = subset_dataset_2(xs,ys,labels,batch_size)
  with tf.GradientTape() as tape:
    pred_l = tf.sigmoid(a * xs_batch + b * ys_batch + c)
    #print(label_batch.shape)
    loss = loss_fn(labels_batch, pred_l)
    Loss.append(loss.numpy())

  dloss_da, dloss_db, dloss_dc = tape.gradient(loss,(a, b,c))

  a.assign_sub(learning_rate*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(learning_rate*dloss_db)  #b = b - alpha*dloss_db
  c.assign_sub(learning_rate*dloss_dc)

Loss

plt.plot(Loss)
plt.show()

"""Sprawdzamy dla pewnego punktu:"""

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter([x],[y],c='b', marker='s')
plt.show()
#print(a,b,c)
tf.sigmoid(a*x + b*y + c).numpy()

tf.sigmoid(a*x + b*y + c).numpy()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

x=5.0
y=4.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()



"""Na uczenie modelu ma najwiekszy wpływ użycie batcha (bez batcha jest podawany cały zbiór uczący), dzięki temu wprowadza, że pewną losowość w procesie uczenia, pomoga to uniknąć utknięcia w minimach lokalnych. Model uczony z minibatchem osiąga lepsze rezultaty jeżeli chodzi o wyniki uczenia(szybszy spadek funkcji błędu oraz mniejszy błąd). Model lepiej i szybciej się uczy gdy mini-batch jest większy niż gdy jest on mniejszy.

Ponadto na proces uczenia modelu ma wpływ ilość epok.
Za mała ilość epok skutkuje niedouczeniem modelu (model nie nauczył się wystarczająco dobrze dostosowywać się do danych treningowych).
Ostatnim sprawdzonym przeze mnie parametrem jest współczynnik uczenia. Po przestawieniu na współczynnik Adam model uczy się lepiej
Jego zbyt duża wartość rowadzi do skakania wokół minimum globalnego przy czym model go nie osiągnie. W przypadku zastosowania zbyt małej wartości współczynnika uczenia proces uczenia jest bardzo wolny na przełomie epok, a model "utyka" w minimach lokalnych.

##Zad 3
"""

def val_train_split(x_dataset, y_dataset,label,subset_size):
    arr = np.arange(len(x_dataset))
    l=len(x_dataset)
    split = int(len(x_dataset)*(1-subset_size))
    #print(split)
    np.random.shuffle(arr)
    x_train = x_dataset[arr[0:split]]
    y_train = y_dataset[arr[0:split]]
    label_train = label[arr[0:split]]
    x_val = x_dataset[arr[split:]]
    y_val = y_dataset[arr[split:]]
    label_val = label[arr[split:]]
    return x_train,y_train,label_train,x_val,y_val,label_val

x_train,y_train,label_train,x_val,y_val,label_val = val_train_split(xs,ys,labels,0.3)

x_train

y_train

label_train

x_val

y_val

label_val

def subset_dataset_2(x_dataset, y_dataset,label,subset_size):
    arr = np.arange(len(x_dataset))
    np.random.shuffle(arr)
    x_train = x_dataset[arr[0:subset_size]]
    y_train = y_dataset[arr[0:subset_size]]
    label_train = label[arr[0:subset_size]]
    return x_train,y_train,label_train

Loss = []
Val_loss =[]
epochs = 1000
learning_rate = 0.01
batch_size = 50
val_split = 0.3
a = tf.Variable(random.random())
b = tf.Variable(random.random())
c = tf.Variable(random.random())
x_tr,y_tr,label_tr,x_val,y_val,label_val = val_train_split(xs,ys,labels,val_split)
for _ in range(epochs):
  xs_batch,ys_batch,labels_batch = subset_dataset_2(x_tr,y_tr,label_tr,batch_size)
  xsv_batch,ysv_batch,labelsv_batch = subset_dataset_2(x_val,y_val,label_val,batch_size)
  with tf.GradientTape() as tape:
    pred_l = tf.sigmoid(a * xs_batch + b * ys_batch + c)
    pred_lv = tf.sigmoid(a * xsv_batch + b * ysv_batch + c)
    #print(label_batch.shape)
    loss = loss_fn(labels_batch, pred_l)
    val_loss = loss_fn(labelsv_batch,pred_lv)
    Loss.append(loss.numpy())
    Val_loss.append(val_loss.numpy())

  dloss_da, dloss_db, dloss_dc = tape.gradient(loss,(a, b,c))

  a.assign_sub(learning_rate*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(learning_rate*dloss_db)  #b = b - alpha*dloss_db
  c.assign_sub(learning_rate*dloss_dc)

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()



"""Sprawdzamy dla pewnego punktu:"""

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter([x],[y],c='b', marker='s')
plt.show()
#print(a,b,c)
tf.sigmoid(a*x + b*y + c).numpy()

tf.sigmoid(a*x + b*y + c).numpy()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

x=5.0
y=4.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

"""#Val_split - 0.4"""



Loss = []
Val_loss =[]
epochs = 1000
learning_rate = 0.01
batch_size = 50
val_split = 0.4
a = tf.Variable(random.random())
b = tf.Variable(random.random())
c = tf.Variable(random.random())
x_tr,y_tr,label_tr,x_val,y_val,label_val = val_train_split(xs,ys,labels,val_split)
for _ in range(epochs):
  xs_batch,ys_batch,labels_batch = subset_dataset_2(x_tr,y_tr,label_tr,batch_size)
  xsv_batch,ysv_batch,labelsv_batch = subset_dataset_2(x_val,y_val,label_val,batch_size)
  with tf.GradientTape() as tape:
    pred_l = tf.sigmoid(a * xs_batch + b * ys_batch + c)
    pred_lv = tf.sigmoid(a * xsv_batch + b * ysv_batch + c)
    #print(label_batch.shape)
    loss = loss_fn(labels_batch, pred_l)
    val_loss = loss_fn(labelsv_batch,pred_lv)
    Loss.append(loss.numpy())
    Val_loss.append(val_loss.numpy())

  dloss_da, dloss_db, dloss_dc = tape.gradient(loss,(a, b,c))

  a.assign_sub(learning_rate*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(learning_rate*dloss_db)  #b = b - alpha*dloss_db
  c.assign_sub(learning_rate*dloss_dc)

Loss

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()

"""Sprawdzamy dla pewnego punktu:"""

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter([x],[y],c='b', marker='s')
plt.show()
#print(a,b,c)
tf.sigmoid(a*x + b*y + c).numpy()

tf.sigmoid(a*x + b*y + c).numpy()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

x=5.0
y=4.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

"""#Val_split - 0.1"""



Loss = []
Val_loss =[]
epochs = 1000
learning_rate = 0.01
batch_size = 50
val_split = 0.1
a = tf.Variable(random.random())
b = tf.Variable(random.random())
c = tf.Variable(random.random())
x_tr,y_tr,label_tr,x_val,y_val,label_val = val_train_split(xs,ys,labels,val_split)
for _ in range(epochs):
  xs_batch,ys_batch,labels_batch = subset_dataset_2(x_tr,y_tr,label_tr,batch_size)
  xsv_batch,ysv_batch,labelsv_batch = subset_dataset_2(x_val,y_val,label_val,batch_size)
  with tf.GradientTape() as tape:
    pred_l = tf.sigmoid(a * xs_batch + b * ys_batch + c)
    pred_lv = tf.sigmoid(a * xsv_batch + b * ysv_batch + c)
    #print(label_batch.shape)
    loss = loss_fn(labels_batch, pred_l)
    val_loss = loss_fn(labelsv_batch,pred_lv)
    Loss.append(loss.numpy())
    Val_loss.append(val_loss.numpy())

  dloss_da, dloss_db, dloss_dc = tape.gradient(loss,(a, b,c))

  a.assign_sub(learning_rate*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(learning_rate*dloss_db)  #b = b - alpha*dloss_db
  c.assign_sub(learning_rate*dloss_dc)

Loss

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()

"""Sprawdzamy dla pewnego punktu:"""

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter([x],[y],c='b', marker='s')
plt.show()
#print(a,b,c)
tf.sigmoid(a*x + b*y + c).numpy()

tf.sigmoid(a*x + b*y + c).numpy()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

x=5.0
y=4.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()

"""Modyfikacja zbioru"""

x_label1 = np.random.normal(4, 1, 1000)
y_label1 = np.random.normal(4, 1, 1000)
x_label2 = np.random.normal(5, 1, 1000)
y_label2 = np.random.normal(6, 1, 1000)

xs = np.append(x_label1, x_label2)
ys = np.append(y_label1, y_label2)
labels = np.asarray([0.]*len(x_label1)+[1.]*len(x_label2))

plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.show()

Loss = []
Val_loss =[]
epochs = 1000
learning_rate = 0.01
batch_size = 50
a = tf.Variable(random.random())
b = tf.Variable(random.random())
c = tf.Variable(random.random())
x_tr,y_tr,label_tr,x_val,y_val,label_val = val_train_split(xs,ys,labels,0.3)
for _ in range(epochs):
  xs_batch,ys_batch,labels_batch = subset_dataset_2(x_tr,y_tr,label_tr,batch_size)
  xsv_batch,ysv_batch,labelsv_batch = subset_dataset_2(x_val,y_val,label_val,batch_size)
  with tf.GradientTape() as tape:
    pred_l = tf.sigmoid(a * xs_batch + b * ys_batch + c)
    pred_lv = tf.sigmoid(a * xsv_batch + b * ysv_batch + c)
    #print(label_batch.shape)
    loss = loss_fn(labels_batch, pred_l)
    val_loss = loss_fn(labelsv_batch,pred_lv)
    Loss.append(loss.numpy())
    Val_loss.append(val_loss.numpy())

  dloss_da, dloss_db, dloss_dc = tape.gradient(loss,(a, b,c))

  a.assign_sub(learning_rate*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(learning_rate*dloss_db)  #b = b - alpha*dloss_db
  c.assign_sub(learning_rate*dloss_dc)

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()

x=5.0
y=4.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
tf.sigmoid(a*x + b*y + c).numpy()



"""Parametr validation split split mówi ile zbioru danych będzie przeznaczone na zbiór validacyjny a ile na zbiór treningowy. im większa wartość validation_split tym mniejszy jest zbiór treningowy. Powyższe wykresy pokazują, ze w wypadku użycia dużego zbioru validacyjnego(validation_split 0.6) model ma problemy się nauczyć."""

