# -*- coding: utf-8 -*-
"""SystemyAI_lab_7_1_Keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fSPMAuLurDoSKxusiKV-smOQVFzHBI0D

Import biblioteki **TensorFlow** ([https://www.tensorflow.org/](https://www.tensorflow.org/)) z której będziemy korzystali w **uczeniu maszynowym**:
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

import keras
from keras.models import Sequential
from keras.layers import Dense

"""**Dwa gangi**

Przetesuj poniższe instrukcje:
"""

[2]*12

[-3]*10+[4]*5

np.append([1,2,3],[4,5])

"""Przygotowujemy zbiór danych:"""

x_label1 = np.random.normal(3, 1, 1000)
y_label1 = np.random.normal(2, 1, 1000)
x_label2 = np.random.normal(7, 1, 1000)
y_label2 = np.random.normal(6, 1, 1000)

xs = np.append(x_label1, x_label2) #tablica wsp. x dla 2000 punktów
ys = np.append(y_label1, y_label2) #tablica wsp. y dla 2000 punktów
labels = np.asarray([0.]*len(x_label1)+[1.]*len(x_label2))

plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.show()

"""##Wersja podstawowa

Definiujemy model:
"""

model = Sequential()

"""Dodajemy **jedną warstwę** (Dense) z **jednym neuronem** (units=1) z **biasem** (use_bias=True) i **liniową funkcją aktywacji** (activation="linear"):"""

model.add(Dense(units = 1, use_bias=True, input_dim=2, activation = "sigmoid"))

"""Definiujemy **optymalizator** i **błąd** (entropia krzyżowa). **Współczynnik uczenia = 0.1**"""

#opt = tf.keras.optimizers.Adam(learning_rate=0.1)
opt = tf.keras.optimizers.SGD(learning_rate=0.1)

model.compile(loss='binary_crossentropy',optimizer=opt)

"""Informacja o modelu:"""

model.summary()

"""Przygotowanie danych:"""

xs[0:10].reshape(-1,1)

xs=xs.reshape(-1,1)
ys=ys.reshape(-1,1)
data_points=np.concatenate([xs,ys],axis=1)
data_points

"""Proces **uczenia**:"""

epochs = 100
h = model.fit(data_points,labels, verbose=1, epochs=epochs)

Loss = h.history['loss']
Loss

"""Sprawdźmy jakie są **wartości wag**:"""

weights = model.get_weights()

print(weights[0])
print(weights[1])    #bias

plt.plot(Loss)
plt.show()

"""Sprawdzamy działanie modelu dla punktu o współrzędnych **x** i **y**:"""

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
model.predict([[x,y]])

"""Liczba epok 50"""



model = Sequential()

model.add(Dense(units = 1, use_bias=True, input_dim=2, activation = "sigmoid"))

#opt = tf.keras.optimizers.Adam(learning_rate=0.1)
opt = tf.keras.optimizers.SGD(learning_rate=0.1)

model.compile(loss='binary_crossentropy',optimizer=opt)

model.summary()

xs[0:10].reshape(-1,1)

xs=xs.reshape(-1,1)
ys=ys.reshape(-1,1)
data_points=np.concatenate([xs,ys],axis=1)
data_points

epochs = 50
h = model.fit(data_points,labels, verbose=1, epochs=epochs)

Loss = h.history['loss']
Loss

weights = model.get_weights()

print(weights[0])
print(weights[1])    #bias

plt.plot(Loss)
plt.show()

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
model.predict([[x,y]])

"""Liczba epok 150"""

model = Sequential()

model.add(Dense(units = 1, use_bias=True, input_dim=2, activation = "sigmoid"))

#opt = tf.keras.optimizers.Adam(learning_rate=0.1)
opt = tf.keras.optimizers.SGD(learning_rate=0.1)

model.compile(loss='binary_crossentropy',optimizer=opt)

model.summary()

xs[0:10].reshape(-1,1)

xs=xs.reshape(-1,1)
ys=ys.reshape(-1,1)
data_points=np.concatenate([xs,ys],axis=1)
data_points

epochs = 150
h = model.fit(data_points,labels, verbose=1, epochs=epochs)

Loss = h.history['loss']
Loss

weights = model.get_weights()

print(weights[0])
print(weights[1])    #bias

plt.plot(Loss)
plt.show()

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
model.predict([[x,y]])

"""współczynnik uczenia 0.01 (SGD)"""

model = Sequential()

model.add(Dense(units = 1, use_bias=True, input_dim=2, activation = "sigmoid"))

#opt = tf.keras.optimizers.Adam(learning_rate=0.1)
opt = tf.keras.optimizers.SGD(learning_rate=0.01)

model.compile(loss='binary_crossentropy',optimizer=opt)

model.summary()

xs[0:10].reshape(-1,1)

xs=xs.reshape(-1,1)
ys=ys.reshape(-1,1)
data_points=np.concatenate([xs,ys],axis=1)
data_points

epochs = 100
h = model.fit(data_points,labels, verbose=1, epochs=epochs)

Loss = h.history['loss']
Loss

weights = model.get_weights()

print(weights[0])
print(weights[1])    #bias

plt.plot(Loss)
plt.show()

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
model.predict([[x,y]])

"""współczynnik uczenia 0.01 (Adam)"""

model = Sequential()

model.add(Dense(units = 1, use_bias=True, input_dim=2, activation = "sigmoid"))

opt = tf.keras.optimizers.Adam(learning_rate=0.1)
#opt = tf.keras.optimizers.SGD(learning_rate=0.1)

model.compile(loss='binary_crossentropy',optimizer=opt)

model.summary()

xs[0:10].reshape(-1,1)

xs=xs.reshape(-1,1)
ys=ys.reshape(-1,1)
data_points=np.concatenate([xs,ys],axis=1)
data_points

epochs = 100
h = model.fit(data_points,labels, verbose=1, epochs=epochs)

Loss = h.history['loss']
Loss

weights = model.get_weights()

print(weights[0])
print(weights[1])    #bias

plt.plot(Loss)
plt.show()

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
model.predict([[x,y]])

"""Batch 100"""

model = Sequential()

model.add(Dense(units = 1, use_bias=True, input_dim=2, activation = "sigmoid"))

#opt = tf.keras.optimizers.Adam(learning_rate=0.1)
opt = tf.keras.optimizers.SGD(learning_rate=0.1)

model.compile(loss='binary_crossentropy',optimizer=opt)

model.summary()

xs[0:10].reshape(-1,1)

xs=xs.reshape(-1,1)
ys=ys.reshape(-1,1)
data_points=np.concatenate([xs,ys],axis=1)
data_points

epochs = 100
h = model.fit(data_points,labels, verbose=1, epochs=epochs,batch_size=100)

Loss = h.history['loss']
Loss

weights = model.get_weights()

print(weights[0])
print(weights[1])    #bias

plt.plot(Loss)
plt.show()

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
model.predict([[x,y]])

"""Batch 200"""

model = Sequential()

model.add(Dense(units = 1, use_bias=True, input_dim=2, activation = "sigmoid"))

#opt = tf.keras.optimizers.Adam(learning_rate=0.1)
opt = tf.keras.optimizers.SGD(learning_rate=0.1)

model.compile(loss='binary_crossentropy',optimizer=opt)

model.summary()

xs[0:10].reshape(-1,1)

xs=xs.reshape(-1,1)
ys=ys.reshape(-1,1)
data_points=np.concatenate([xs,ys],axis=1)
data_points

epochs = 100
h = model.fit(data_points,labels, verbose=1, epochs=epochs,batch_size=200)

Loss = h.history['loss']
Loss

weights = model.get_weights()

print(weights[0])
print(weights[1])    #bias

plt.plot(Loss)
plt.show()

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
model.predict([[x,y]])

"""Batch 400"""

model = Sequential()

model.add(Dense(units = 1, use_bias=True, input_dim=2, activation = "sigmoid"))

#opt = tf.keras.optimizers.Adam(learning_rate=0.1)
opt = tf.keras.optimizers.SGD(learning_rate=0.1)

model.compile(loss='binary_crossentropy',optimizer=opt)

model.summary()

xs[0:10].reshape(-1,1)

xs=xs.reshape(-1,1)
ys=ys.reshape(-1,1)
data_points=np.concatenate([xs,ys],axis=1)
data_points

epochs = 100
h = model.fit(data_points,labels, verbose=1, epochs=epochs,batch_size=400)

Loss = h.history['loss']
Loss

weights = model.get_weights()

print(weights[0])
print(weights[1])    #bias

plt.plot(Loss)
plt.show()

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
model.predict([[x,y]])

"""##Najlepszy model"""

model = Sequential()

model.add(Dense(units = 1, use_bias=True, input_dim=2, activation = "sigmoid"))

opt = tf.keras.optimizers.Adam(learning_rate=0.1)
#opt = tf.keras.optimizers.SGD(learning_rate=0.1)

model.compile(loss='binary_crossentropy',optimizer=opt)

model.summary()

xs[0:10].reshape(-1,1)

xs=xs.reshape(-1,1)
ys=ys.reshape(-1,1)
data_points=np.concatenate([xs,ys],axis=1)
data_points

epochs = 150
h = model.fit(data_points,labels, verbose=1, epochs=epochs,batch_size=150)

Loss = h.history['loss']
Loss

weights = model.get_weights()

print(weights[0])
print(weights[1])    #bias

plt.plot(Loss)
plt.show()

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()
model.predict([[x,y]])

"""Na uczenie modelu ma najwiekszy wpływ użycie batcha (bez batcha jest podawany cały zbiór uczący), dzięki temu wprowadza, że pewną losowość w procesie uczenia, pomoga to uniknąć utknięcia w minimach lokalnych. Model uczony z minibatchem osiąga lepsze rezultaty jeżeli chodzi o wyniki uczenia(szybszy spadek funkcji błędu oraz mniejszy błąd). Model lepiej i szybciej się uczy gdy mini-batch jest większy niż gdy jest on mniejszy.

Ponadto na proces uczenia modelu ma wpływ ilość epok.
Za mała ilość epok skutkuje niedouczeniem modelu (model nie nauczył się wystarczająco dobrze dostosowywać się do danych treningowych).
Ostatnim sprawdzonym przeze mnie parametrem jest współczynnik uczenia. Po przestawieniu na współczynnik Adam model uczy się lepiej
Jego zbyt duża wartość rowadzi do skakania wokół minimum globalnego przy czym model go nie osiągnie. W przypadku zastosowania zbyt małej wartości współczynnika uczenia proces uczenia jest bardzo wolny na przełomie epok.

##Wersja ze zbiorami treningowym i walidacyjnym

Wartości domyślne
"""

model = Sequential()

model.add(Dense(units = 1, use_bias=True, input_dim=2, activation = "sigmoid"))

#opt = tf.keras.optimizers.Adam(learning_rate=0.1)
opt = tf.keras.optimizers.SGD(learning_rate=0.1)

model.compile(loss='binary_crossentropy',optimizer=opt)
#model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])

model.summary()

epochs = 100
h = model.fit(data_points,labels,validation_split=0.2,verbose=1, epochs=epochs,batch_size=100)

h.history.keys()

Loss = h.history['loss']
Val_loss = h.history['val_loss']

plt.plot(Loss,label="training loss")
plt.plot(Val_loss,label="validation loss")
plt.legend()
plt.show()

"""#Validation_split 0.6"""



model = Sequential()

model.add(Dense(units = 1, use_bias=True, input_dim=2, activation = "sigmoid"))

#opt = tf.keras.optimizers.Adam(learning_rate=0.1)
opt = tf.keras.optimizers.SGD(learning_rate=0.1)

model.compile(loss='binary_crossentropy',optimizer=opt)
#model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])

model.summary()

epochs = 100
h = model.fit(data_points,labels,validation_split=0.6,verbose=1, epochs=epochs,batch_size=100)

h.history.keys()

Loss = h.history['loss']
Val_loss = h.history['val_loss']

plt.plot(Loss,label="training loss")
plt.plot(Val_loss,label="validation loss")
plt.legend()
plt.show()

"""#Validation_split 0.1"""



model = Sequential()

model.add(Dense(units = 1, use_bias=True, input_dim=2, activation = "sigmoid"))

#opt = tf.keras.optimizers.Adam(learning_rate=0.1)
opt = tf.keras.optimizers.SGD(learning_rate=0.1)

model.compile(loss='binary_crossentropy',optimizer=opt)
#model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])

model.summary()

epochs = 100
h = model.fit(data_points,labels,validation_split=0.1,verbose=1, epochs=epochs,batch_size=100)

h.history.keys()

Loss = h.history['loss']
Val_loss = h.history['val_loss']

plt.plot(Loss,label="training loss")
plt.plot(Val_loss,label="validation loss")
plt.legend()
plt.show()



"""Parametr validation split split mówi ile zbioru danych będzie przeznaczone na zbiór validacyjny a ile na zbiór treningowy. im większa wartość validation_split tym mniejszy jest zbiór treningowy. Powyższe wykresy pokazują, ze w wypadku użycia dużego zbioru validacyjnego(validation_split 0.6) model ma problemy się nauczyć."""

