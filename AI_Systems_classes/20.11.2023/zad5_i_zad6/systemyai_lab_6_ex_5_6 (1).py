# -*- coding: utf-8 -*-
"""SystemyAI_lab_6_ex_5_6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rjEHOVcD_4gBIrUoSC3tMEEjAscH4N5M

ZAD 5
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df = pd.read_csv('Boston.csv')
print(df)

df.head()

rm=df.iloc[:,6]

rm

medv=df.iloc[:,14]

medv

plt.figure(figsize=(20, 5))

features = ['rm']
target = df['medv']

for i, col in enumerate(features):
    plt.subplot(1, len(features) , i+1)
    x = df[col]
    y = target
    plt.scatter(x, y, marker='o')
    plt.title(col)
    plt.xlabel(col)
    plt.ylabel('medv')

real_rm = np.array(rm)
real_medv = np.array(medv)

real_rm

import tensorflow as tf
import keras
from keras.layers import Dense
from keras.models import Sequential

model=Sequential()

model.add(Dense(units = 1, use_bias=True, input_dim=1, activation = "linear"))

opt = tf.keras.optimizers.Adam(learning_rate=0.1)

model.compile(loss='MSE',optimizer=opt)

model.summary()

epochs = 1500
h = model.fit(real_rm,real_medv, verbose=1, epochs=epochs, batch_size=150, validation_split=0.3)

#Loss = h.history['loss']
#Loss

plt.scatter(np.arange(epochs),h.history['loss'])
plt.scatter(np.arange(epochs),h.history['val_loss'],c='r')
plt.show()

weights = model.get_weights()
print(weights[0][0][0])
print(weights[1][0])

max = np.max(rm)
min = np.min(rm)
X = np.linspace(min, max, num=10)
plt.plot(X,weights[0][0][0]*X+weights[1][0],c='r')
plt.scatter(rm,medv,c="b")
plt.show()

"""Zad 6"""



model=Sequential()

model.add(Dense(units = 1, use_bias=True, input_dim=1, activation = "linear"))
model.add(Dense(units = 5, activation = "relu"))
model.add(Dense(units = 3, activation = "relu"))

opt = tf.keras.optimizers.Adam(learning_rate=0.1)

model.compile(loss='MSE',optimizer=opt)

model.summary()

epochs = 1500
h = model.fit(real_rm,real_medv, verbose=1, epochs=epochs, batch_size=150, validation_split=0.3)

#Loss = h.history['loss']
#Loss

plt.scatter(np.arange(epochs),h.history['loss'])
plt.scatter(np.arange(epochs),h.history['val_loss'],c='r')
plt.show()

weights = model.get_weights()
print(weights[0][0][0])
print(weights[1][0])

max = np.max(rm)
min = np.min(rm)
X = np.linspace(min, max, num=10)
plt.plot(X,weights[0][0][0]*X+weights[1][0],c='r')
plt.scatter(rm,medv,c="b")
plt.show()



model=Sequential()

model.add(Dense(units = 1, use_bias=True, input_dim=1, activation = "linear"))
model.add(Dense(units = 5, activation = "relu"))
model.add(Dense(units = 3, activation = "relu"))

opt = tf.keras.optimizers.Adam(learning_rate=0.1)

model.compile(loss='BinaryCrossentropy',optimizer=opt)

model.summary()

epochs = 3000
h = model.fit(real_rm,real_medv, verbose=1, epochs=epochs, batch_size=150, validation_split=0.3)

#Loss = h.history['loss']
#Loss

plt.scatter(np.arange(epochs),h.history['loss'])
plt.scatter(np.arange(epochs),h.history['val_loss'],c='r')
plt.show()

weights = model.get_weights()
print(weights[0][0][0])
print(weights[1][0])

max = np.max(rm)
min = np.min(rm)
X = np.linspace(min, max, num=10)
plt.plot(X,weights[0][0][0]*X+weights[1][0],c='r')
plt.scatter(rm,medv,c="b")
plt.show()



"""Sieć z zadania 6 osiąga, DUŻO gorsze wyniki niż sieć zadania z 5, nie uczy się albo uczy się tylko na początku potem już nie (model jest zbyt rozbudowany)"""

