# -*- coding: utf-8 -*-
"""SystemyAI_lab_6_1_Batch_Bostono_SGD_Minibatch_Batch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/181fmXUO8AstE_MWoQVoLESE3SDUVT_1d

Import biblioteki **TensorFlow** ([https://www.tensorflow.org/](https://www.tensorflow.org/)) z której będziemy korzystali w **uczeniu maszynowym**:
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
df = pd.read_csv('Boston.csv')
print(df)

df.head()

rm=df.iloc[:,6]

rm

medv=df.iloc[:,14]

medv

plt.figure(figsize=(20, 5))

features = ['rm']
target = df['medv']

for i, col in enumerate(features):
    plt.subplot(1, len(features) , i+1)
    x = df[col]
    y = target
    plt.scatter(x, y, marker='o')
    plt.title(col)
    plt.xlabel(col)
    plt.ylabel('medv')

plt.show()

rm

df.corr()

df.corr()

real_x = np.array(rm)
real_y = np.array(medv)

real_x

real_y

"""**Batch Stochastic Gradient Descent** - wykorzystujemy **cały zbiór danych**"""



"""Definicja błędu:"""

def loss_fn(real_y, pred_y):
    return tf.reduce_mean((real_y - pred_y)**2)

import random

Loss = []
epochs = 2000
learning_rate = 0.02

a = tf.Variable(random.random())
b = tf.Variable(random.random())

for _ in range(epochs):

  with tf.GradientTape() as tape:
    pred_y = a * real_x + b
    #print(pred_y)
    loss = loss_fn(real_y, pred_y)
    Loss.append(loss.numpy())
    grad_a, grad_b = tape.gradient(loss,(a, b))

  a.assign_sub(learning_rate*grad_a)
  b.assign_sub(learning_rate*grad_b)

np.max(Loss),np.min(Loss)

print(a.numpy())
print(b.numpy())

plt.scatter(np.arange(epochs),Loss)
plt.show()

max = np.max(rm)
min = np.min(rm)

X = np.linspace(min, max, num=10)
plt.plot(X,a.numpy()*X+b.numpy(),c='r')
plt.scatter(rm,medv,c="b")
plt.show()

"""**Mini-batch Stochastic Gradient Descent** - wykorzystujemy **część zbióru danych**

Definiujemy tablicę:
"""

arr = np.arange(10)
arr

"""Mieszamy zawartość tablicy:"""

np.random.shuffle(arr)
arr

"""Funkcja do przetestowania:"""

def subset_dataset(x_dataset, y_dataset, subset_size):
    arr = np.arange(len(x_dataset))
    np.random.shuffle(arr)
    x_train = x_dataset[arr[0:subset_size]]
    y_train = y_dataset[arr[0:subset_size]]
    return x_train,y_train

"""Uzupełnik poniższy kod, tak aby możliwe było testowanie różnych wielkości próbki treningowwej."""

def mini_batch_stochastic_gradient_descent(batch_size):
  Loss = []
  epochs = 2000
  learning_rate = 0.02
  batch_size = batch_size     #wielkość zbioru wykorzystanego do treningu

  a = tf.Variable(random.random())
  b = tf.Variable(random.random())

  for i in range(epochs):

    real_x_batch, real_y_batch = subset_dataset(real_x,real_y,batch_size)

    with tf.GradientTape() as tape:
      pred_y = a * real_x_batch + b
      loss = loss_fn(real_y_batch, pred_y)
      Loss.append(loss.numpy())

    dloss_da, dloss_db = tape.gradient(loss,(a, b))

    a.assign_sub(learning_rate*dloss_da)  #a = a - alpha*dloss_da
    b.assign_sub(learning_rate*dloss_db)  #b = b - alpha*dloss_db
  print("last one loss", str(loss))

  plt.scatter(np.arange(epochs),Loss)
  plt.show()
  max = np.max(rm)
  min = np.min(rm)
  X = np.linspace(min, max, num=10)
  plt.plot(X,a.numpy()*X+b.numpy(),c='r')
  plt.scatter(rm,medv,c="b")
  plt.show()

mini_batch_stochastic_gradient_descent(1)

mini_batch_stochastic_gradient_descent(10)

mini_batch_stochastic_gradient_descent(100)

"""Wykres zmian błędu:"""

#do uzupełnienia

def subset_dataset(x_dataset, y_dataset, subset_size):
    arr = np.arange(len(x_dataset))
    np.random.shuffle(arr)
    x_train = x_dataset[arr[0:subset_size]]
    y_train = y_dataset[arr[0:subset_size]]
    return x_train,y_train

def loss_fn(real_y, pred_y):
    return tf.reduce_mean((real_y - pred_y)**2)

Loss = []
epochs = 2000
learning_rate = 0.01
batch_size = 50
a = tf.Variable(random.random())
b = tf.Variable(random.random())
for _ in range(epochs):
  real_rm_batch,real_medv_batch = subset_dataset(real_x,real_y,batch_size)
  with tf.GradientTape() as tape:
    pred_medv = a * real_rm_batch + b
    loss = loss_fn(real_medv_batch, pred_medv)
    Loss.append(loss.numpy())

  dloss_da, dloss_db = tape.gradient(loss,(a, b))

  a.assign_sub(learning_rate*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(learning_rate*dloss_db)  #b = b - alpha*dloss_db

plt.scatter(np.arange(epochs),Loss)
  plt.show()

max = np.max(rm)
  min = np.min(rm)
  X = np.linspace(min, max, num=10)
  plt.plot(X,a.numpy()*X+b.numpy(),c='r')
  plt.scatter(rm,medv,c="b")
  plt.show()

"""##Podsumowanie

Na uczenie modelu ma najwiekszy wpływ użycie batcha (bez batcha jest podawany cały zbiór uczący), małe batche mogą przyspieszyć proces uczenia, ponieważ aktualizacje wag modelu są wykonywane częściej. Dzięki temu wprowadza to pewną losowość w procesie uczenia, pomoga uniknąć utknięcia w minimach lokalnych. Model uczony z minibatchem osiąga lepsze rezultaty jeżeli chodzi o wyniki uczenia(lepiej znaleziona prosta) oraz mniejszy błąd. Model lepiej i szybciej się uczy gdy mini-batch jest większy niż gdy jest on mniejszy.
Ponadto na proces uczenia modelu ma wpływ ilość epok.
Za mała ilość epok skutkuje niedouczeniem modelu (model nie nauczył się wystarczająco dobrze dostosowywać się do danych treningowych), zaś gdy ilość epok jest zbyt duża następuje przeuczenie modelu (model nieuogulnia zgromadzonej wiedzy tylko "uczy się na pamięć" zbioru treningowego co sprawia, że jest nieskuteczny lub mało skuteczny dla nowych danych).
Ostatnim sprawdzonym przeze mnie parametrem jest współczynnik uczenia.
Jego zbyt duża wartość rowadzi do skakania wokół minimum globalnego przy czym model go nie osiągnie. W przypadku zastosowania zbyt małej wartości współczynnika uczenia proces uczenia jest bardzo wolny, a model "utyka" w minimach lokalnych.
"""

