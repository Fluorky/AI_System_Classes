# -*- coding: utf-8 -*-
"""SystemyAI_lab_5_część_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11wUl06ahwTXRSjsrJ00C8emtnwjQqXaf

Import biblioteki **TensorFlow** ([https://www.tensorflow.org/](https://www.tensorflow.org/)) z której będziemy korzystali w **uczeniu maszynowym**:
"""

import tensorflow as tf

"""**Gradient**

Możliwe jest wyliczenie gradientu dowolnego wyrażenia różniczkowalnego. Wykorzystujemy do tego metodę **tf.GradientTape()**

Funkcja **jednej zmiennej**:
"""

x = tf.Variable(4.0)
with tf.GradientTape() as tape:
    f = x**3                     #definicja funkcji f(x)=x^3
    df_dx = tape.gradient(f, x)  #gradient 'f' ze względu na zmienną 'x'

df_dx.numpy()

"""Funkcja **dwóch zmiennych**:"""

x = tf.Variable(4.0)
y = tf.Variable(3.0)

with tf.GradientTape() as tape:
    f = x**3+y**2                         #definicja funkcji f(x,y)=x^3+y^2
    df_dx,df_dy = tape.gradient(f,(x,y))  #gradient 'f' ze względu na zmienną 'x' i ze względu na zmienną 'y'

print(df_dx)
print(df_dy)

"""Przykład z **prezentacji**:"""

x = tf.Variable(3.0)
y = tf.Variable(2.0)

with tf.GradientTape() as tape:
    f = (x**2)*y                          #definicja funkcji f(x,y)=x^2*y
    df_dx,df_dy = tape.gradient(f,(x,y))  #gradient 'f' ze względu na zmienną 'x' i ze względu na zmienną 'y'

print(df_dx)
print(df_dy)

"""Trochę skomplikujemy:"""

x = tf.Variable([3.0,2.0])

with tf.GradientTape() as tape:
    f = (x**3)                  #definicja funkcji f(x)=x^3
    df_dx = tape.gradient(f,x)  #gradient 'f' ze względu na zmienną 'x'

print(df_dx)

"""I jeszcze trochę skomplikujemy:"""

x = tf.Variable([3.0,2.0])
y = tf.Variable([1.0,0.0])

with tf.GradientTape() as tape:
    f = (x**3)+y**2                       #definicja funkcji f(x)=x^3+y^2
    df_dx,df_dy = tape.gradient(f,(x,y))  #gradient 'f' ze względu na zmienną 'x'

print(df_dx)
print(df_dy)

"""Zmienne mogą być zastąpione przez tensory, wówczas konieczne jest **rejestrowanie wprost** operacji zastosowanych do tych sensorów. Służy do tego metoda **watch()**. W przypkadku zmiennych operacje są rejestrowane automatycznie."""

x = tf.random.normal([2])
y = tf.random.normal([2])

print(x)
print(y)

with tf.GradientTape() as tape:
    tape.watch(x)
    tape.watch(y)
    f = (x**3)+y**2                       #definicja funkcji f(x)=x^3+y^2
    df_dx,df_dy = tape.gradient(f,(x,y))  #gradient 'f' ze względu na zmienną 'x'

print(df_dx)
print(df_dy)

import matplotlib.pyplot as plt
import numpy as np

number_of_points = 1000
x_point = []
y_point = []

a = 0.22
b = 0.78

for i in range(number_of_points):
    x = np.random.normal(0.0,0.5)
    y = (a*x+b)+np.random.normal(0.0,0.1)
    x_point.append(x)
    y_point.append(y)

plt.scatter(x_point,y_point,c='b')
plt.show()

x_point

real_x = np.array(x_point)
real_y = np.array(y_point)

"""Definicja błędu:"""

def loss_fn(real_y, pred_y):
    return tf.reduce_mean((real_y - pred_y)**2)

x = tf.constant([1.0, 2.0, 3.0, 4.0])
tf.reduce_mean(x).numpy()

import random

"""TODO"""

Loss = []
epochs = 1000
learning_rate = 0.001

a = tf.Variable(random.random())
b = tf.Variable(random.random())

for _ in range(epochs):
  with tf.GradientTape() as tape:
    pred_y = a * real_x + b
    loss = loss_fn(real_y, pred_y)
    Loss.append(loss.numpy())

  dloss_da, dloss_db = tape.gradient(loss,(a, b))

  a.assign_sub(learning_rate*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(learning_rate*dloss_db)  #b = b - alpha*dloss_db

"""Wykres zmian błędu:"""

plt.scatter(np.arange(epochs),Loss)
plt.show()

max = np.max(x_point)
min = np.min(x_point)

X = np.linspace(min, max, num=10)
plt.plot(X,a.numpy()*X+b.numpy(),c='r')
plt.scatter(x_point,y_point,c="b")
plt.show()



