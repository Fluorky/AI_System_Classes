# -*- coding: utf-8 -*-
"""SAI_8_zad5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OAJHgPpcTVe0dBYx3P0s6bGRI_iYnAre

#Exercise 5
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import random

import keras
from keras.models import Sequential
from keras.layers import Dense

"""Three gangs

Softmax regresion
"""

s = [0.2, 0.1, 0.6, 0.1]
exps = [np.exp(i) for i in s]
sum_of_exps = sum(exps)
softmax = [j/sum_of_exps for j in exps]

"""Dataset:"""



x_label0 = np.random.normal(1, 1.5, (1000, 1))
y_label0 = np.random.normal(1, 1.5, (1000, 1))
x_label1 = np.random.normal(5, 1.5, (1000, 1))
y_label1 = np.random.normal(4, 1.5, (1000, 1))
x_label2 = np.random.normal(8, 1.5, (1000, 1))
y_label2 = np.random.normal(0, 1.5, (1000, 1))

data_label0 = np.concatenate([x_label0, y_label0],axis=1)
data_label1 = np.concatenate([x_label1, y_label1],axis=1)
data_label2 = np.concatenate([x_label2, y_label2],axis=1)
points = np.concatenate([data_label0, data_label1, data_label2],axis=0)



"""Kodowanie one-hot"""

labels = np.array([[1., 0., 0.]] * len(data_label0) + [[0., 1., 0.]] * len(data_label1) + [[0.,0., 1.]] * len(data_label2))

points.shape,labels.shape

plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.show()



x_label1

#def loss_fn_grad(y, y_model):
# return tf.reduce_mean(-y*tf.math.log(y_model)-(1-y)*tf.math.log(1-y_model))

def split_dataset(data_points, label,subset_size):
    arr = np.arange(len(data_points))
    l=len(data_points)
    s=int(subset_size*l)
    np.random.shuffle(arr)
    data_points_val =data_points[arr[0:s]]
    label_val = label[arr[0:s]]
    data_points_train = data_points[arr[:int(l*(1-subset_size))]]
    label_train = label[arr[:int(l*(1-subset_size))]]

    return data_points_train,label_train,data_points_val,label_val



def subset_dataset(x_dataset, y_dataset,label,subset_size):
    arr = np.arange(len(x_dataset))
    np.random.shuffle(arr)
    x_train = x_dataset[arr[0:subset_size]]
    y_train = y_dataset[arr[0:subset_size]]
    label_train = label[arr[0:subset_size]]
    return x_train,y_train,label_train

def subset_dataset_concatenated(data,label,subset_size):
    arr = np.arange(len(data))
    np.random.shuffle(arr)
    data_train = data[arr[0:subset_size]]
    label_train = label[arr[0:subset_size]]
    return data_train,label_train

labels.shape



Loss = []
Val_loss = []
epochs = 5000
learning_rate = 0.1
batch_size = 20

w = tf.Variable(np.random.random((2, 3)))
b = tf.Variable(np.random.random((3)))
data = points
data_train,label_train,data_val,label_val = split_dataset(data,labels,0.2)
for _ in range(epochs):

  data_batch,labels_batch = subset_dataset_concatenated(data_train,label_train,batch_size)
  data_val_batch,labels_val_batch = subset_dataset_concatenated(data_val,label_val,batch_size)

  with tf.GradientTape() as tape:

    pred_l=tf.nn.softmax(tf.matmul(data_batch, w) + b)
    pred_l_val=tf.nn.softmax(tf.matmul(data_val_batch, w) + b)


    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_batch, pred_l)
    Loss.append(loss.numpy())

    val_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_val_batch, pred_l_val)
    Val_loss.append(val_loss.numpy())

    print("loss",loss,"val_loss",val_loss)

  dloss_dw,dloss_db = tape.gradient(loss, [w, b])


  w.assign_sub(learning_rate*dloss_dw )
  b.assign_sub(learning_rate*dloss_db )

print(data_train.size,label_train.size,data_val.size,label_val.size)

print(data_train.shape,label_train.shape,data_val.shape,label_val.shape)

np.max(Loss),np.min(Loss)

np.max(Val_loss),np.min(Val_loss)

print(w.numpy())
print(b.numpy())

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()



x=3.0
y=2.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=7.0
y=6.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=4.0
y=5.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

"""#Hiperparametria

##Learning rate 0.01
"""



Loss = []
Val_loss = []
epochs = 5000
learning_rate = 0.01
batch_size = 20

w = tf.Variable(np.random.random((2, 3)))
b = tf.Variable(np.random.random((3)))
data = points
data_train,label_train,data_val,label_val = split_dataset(data,labels,0.2)
for _ in range(epochs):

  data_batch,labels_batch = subset_dataset_concatenated(data_train,label_train,batch_size)
  data_val_batch,labels_val_batch = subset_dataset_concatenated(data_val,label_val,batch_size)

  with tf.GradientTape() as tape:

    pred_l=tf.nn.softmax(tf.matmul(data_batch, w) + b)
    pred_l_val=tf.nn.softmax(tf.matmul(data_val_batch, w) + b)


    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_batch, pred_l)
    Loss.append(loss.numpy())

    val_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_val_batch, pred_l_val)
    Val_loss.append(val_loss.numpy())

    print("loss",loss,"val_loss",val_loss)

  dloss_dw,dloss_db = tape.gradient(loss, [w, b])


  w.assign_sub(learning_rate*dloss_dw )
  b.assign_sub(learning_rate*dloss_db )

print(data_train.size,label_train.size,data_val.size,label_val.size)

print(data_train.shape,label_train.shape,data_val.shape,label_val.shape)

np.max(Loss),np.min(Loss)

np.max(Val_loss),np.min(Val_loss)

print(w.numpy())
print(b.numpy())

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()



x=3.0
y=2.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=7.0
y=6.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=4.0
y=5.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

"""Learning rate 0.001"""



Loss = []
Val_loss = []
epochs = 5000
learning_rate = 0.001
batch_size = 20

w = tf.Variable(np.random.random((2, 3)))
b = tf.Variable(np.random.random((3)))
data = points
data_train,label_train,data_val,label_val = split_dataset(data,labels,0.2)
for _ in range(epochs):

  data_batch,labels_batch = subset_dataset_concatenated(data_train,label_train,batch_size)
  data_val_batch,labels_val_batch = subset_dataset_concatenated(data_val,label_val,batch_size)

  with tf.GradientTape() as tape:

    pred_l=tf.nn.softmax(tf.matmul(data_batch, w) + b)
    pred_l_val=tf.nn.softmax(tf.matmul(data_val_batch, w) + b)


    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_batch, pred_l)
    Loss.append(loss.numpy())

    val_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_val_batch, pred_l_val)
    Val_loss.append(val_loss.numpy())

    print("loss",loss,"val_loss",val_loss)

  dloss_dw,dloss_db = tape.gradient(loss, [w, b])


  w.assign_sub(learning_rate*dloss_dw )
  b.assign_sub(learning_rate*dloss_db )

print(data_train.size,label_train.size,data_val.size,label_val.size)

print(data_train.shape,label_train.shape,data_val.shape,label_val.shape)

np.max(Loss),np.min(Loss)

np.max(Val_loss),np.min(Val_loss)

print(w.numpy())
print(b.numpy())

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()



x=3.0
y=2.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=7.0
y=6.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=4.0
y=5.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

"""#Number of epchos - 100"""



Loss = []
Val_loss = []
epochs = 100
learning_rate = 0.1
batch_size = 20

w = tf.Variable(np.random.random((2, 3)))
b = tf.Variable(np.random.random((3)))
data = points
data_train,label_train,data_val,label_val = split_dataset(data,labels,0.2)
for _ in range(epochs):

  data_batch,labels_batch = subset_dataset_concatenated(data_train,label_train,batch_size)
  data_val_batch,labels_val_batch = subset_dataset_concatenated(data_val,label_val,batch_size)

  with tf.GradientTape() as tape:

    pred_l=tf.nn.softmax(tf.matmul(data_batch, w) + b)
    pred_l_val=tf.nn.softmax(tf.matmul(data_val_batch, w) + b)


    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_batch, pred_l)
    Loss.append(loss.numpy())

    val_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_val_batch, pred_l_val)
    Val_loss.append(val_loss.numpy())

    print("loss",loss,"val_loss",val_loss)

  dloss_dw,dloss_db = tape.gradient(loss, [w, b])


  w.assign_sub(learning_rate*dloss_dw )
  b.assign_sub(learning_rate*dloss_db )

print(data_train.size,label_train.size,data_val.size,label_val.size)

print(data_train.shape,label_train.shape,data_val.shape,label_val.shape)

np.max(Loss),np.min(Loss)

np.max(Val_loss),np.min(Val_loss)

print(w.numpy())
print(b.numpy())

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()



x=3.0
y=2.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=7.0
y=6.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=4.0
y=5.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

"""#Number of epochs - 3000"""



Loss = []
Val_loss = []
epochs = 3000
learning_rate = 0.1
batch_size = 20

w = tf.Variable(np.random.random((2, 3)))
b = tf.Variable(np.random.random((3)))
data = points
data_train,label_train,data_val,label_val = split_dataset(data,labels,0.2)
for _ in range(epochs):

  data_batch,labels_batch = subset_dataset_concatenated(data_train,label_train,batch_size)
  data_val_batch,labels_val_batch = subset_dataset_concatenated(data_val,label_val,batch_size)

  with tf.GradientTape() as tape:

    pred_l=tf.nn.softmax(tf.matmul(data_batch, w) + b)
    pred_l_val=tf.nn.softmax(tf.matmul(data_val_batch, w) + b)


    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_batch, pred_l)
    Loss.append(loss.numpy())

    val_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_val_batch, pred_l_val)
    Val_loss.append(val_loss.numpy())

    print("loss",loss,"val_loss",val_loss)

  dloss_dw,dloss_db = tape.gradient(loss, [w, b])


  w.assign_sub(learning_rate*dloss_dw )
  b.assign_sub(learning_rate*dloss_db )

print(data_train.size,label_train.size,data_val.size,label_val.size)

print(data_train.shape,label_train.shape,data_val.shape,label_val.shape)

np.max(Loss),np.min(Loss)

np.max(Val_loss),np.min(Val_loss)

print(w.numpy())
print(b.numpy())

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()



x=3.0
y=2.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=7.0
y=6.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=4.0
y=5.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

"""#Minibatch

#Batch size - 10
"""



Loss = []
Val_loss = []
epochs = 5000
learning_rate = 0.1
batch_size = 10

w = tf.Variable(np.random.random((2, 3)))
b = tf.Variable(np.random.random((3)))
data = points
data_train,label_train,data_val,label_val = split_dataset(data,labels,0.2)
for _ in range(epochs):

  data_batch,labels_batch = subset_dataset_concatenated(data_train,label_train,batch_size)
  data_val_batch,labels_val_batch = subset_dataset_concatenated(data_val,label_val,batch_size)

  with tf.GradientTape() as tape:

    pred_l=tf.nn.softmax(tf.matmul(data_batch, w) + b)
    pred_l_val=tf.nn.softmax(tf.matmul(data_val_batch, w) + b)


    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_batch, pred_l)
    Loss.append(loss.numpy())

    val_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_val_batch, pred_l_val)
    Val_loss.append(val_loss.numpy())

    print("loss",loss,"val_loss",val_loss)

  dloss_dw,dloss_db = tape.gradient(loss, [w, b])


  w.assign_sub(learning_rate*dloss_dw )
  b.assign_sub(learning_rate*dloss_db )

print(data_train.size,label_train.size,data_val.size,label_val.size)

print(data_train.shape,label_train.shape,data_val.shape,label_val.shape)

np.max(Loss),np.min(Loss)

np.max(Val_loss),np.min(Val_loss)

print(w.numpy())
print(b.numpy())

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()



x=3.0
y=2.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=7.0
y=6.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=4.0
y=5.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

"""#Batch size - 100"""



Loss = []
Val_loss = []
epochs = 5000
learning_rate = 0.1
batch_size = 100

w = tf.Variable(np.random.random((2, 3)))
b = tf.Variable(np.random.random((3)))
data = points
data_train,label_train,data_val,label_val = split_dataset(data,labels,0.2)
for _ in range(epochs):

  data_batch,labels_batch = subset_dataset_concatenated(data_train,label_train,batch_size)
  data_val_batch,labels_val_batch = subset_dataset_concatenated(data_val,label_val,batch_size)

  with tf.GradientTape() as tape:

    pred_l=tf.nn.softmax(tf.matmul(data_batch, w) + b)
    pred_l_val=tf.nn.softmax(tf.matmul(data_val_batch, w) + b)


    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_batch, pred_l)
    Loss.append(loss.numpy())

    val_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_val_batch, pred_l_val)
    Val_loss.append(val_loss.numpy())

    print("loss",loss,"val_loss",val_loss)

  dloss_dw,dloss_db = tape.gradient(loss, [w, b])


  w.assign_sub(learning_rate*dloss_dw )
  b.assign_sub(learning_rate*dloss_db )

print(data_train.size,label_train.size,data_val.size,label_val.size)

print(data_train.shape,label_train.shape,data_val.shape,label_val.shape)

np.max(Loss),np.min(Loss)

np.max(Val_loss),np.min(Val_loss)

print(w.numpy())
print(b.numpy())

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()



x=3.0
y=2.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=7.0
y=6.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=4.0
y=5.0
plt.scatter(x_label0, y_label0, c='green', marker='x', s=20)
plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='purple', marker='x', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

"""Najlepsze wyniki otrzymałem dla współczynnika uczenia 0.1, liczby epok 5000, batcha równego 20, najgorsze dla współczynnika uczenia 0.001, liczby epok 100, batcha równego 100.

I got the best results for a learning rate of 0.1, a number of epochs of 5000, a batch of 20, and the worst for a learning rate of 0.001, a number of epochs of 100, a batch of 100.
"""



