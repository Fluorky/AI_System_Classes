# -*- coding: utf-8 -*-
"""SAI_8_ex4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mZQPaqVV_6ntAh21eJvK45fUhy_jrvw_

#Exercise 1
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import random

import keras
from keras.models import Sequential
from keras.layers import Dense

"""Two gangs

Dataset:
"""

[0]*10+[1]*10

x_label1 = np.random.normal(3, 1, 1000)
y_label1 = np.random.normal(2, 1, 1000)
x_label2 = np.random.normal(7, 1, 1000)
y_label2 = np.random.normal(6, 1, 1000)

xs = np.append(x_label1, x_label2)
ys = np.append(y_label1, y_label2)
labels = np.asarray([0.]*len(x_label1)+[1.]*len(x_label2))
labels

plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='brown', marker='1', s=20)
plt.show()

x_label1

def loss_fn_grad(y, y_model):
 return tf.reduce_mean(-y*tf.math.log(y_model)-(1-y)*tf.math.log(1-y_model))

def split_dataset(data_points, label,subset_size):
    arr = np.arange(len(data_points))
    l=len(data_points)
    s=int(subset_size*l)
    np.random.shuffle(arr)
    data_points_val =data_points[arr[0:s]]
    label_val = label[arr[0:s]]
    data_points_train = data_points[arr[:int(l*(1-subset_size))]]
    label_train = label[arr[:int(l*(1-subset_size))]]

    return data_points_train,label_train,data_points_val,label_val



def subset_dataset(x_dataset, y_dataset,label,subset_size):
    arr = np.arange(len(x_dataset))
    np.random.shuffle(arr)
    x_train = x_dataset[arr[0:subset_size]]
    y_train = y_dataset[arr[0:subset_size]]
    label_train = label[arr[0:subset_size]]
    return x_train,y_train,label_train

def subset_dataset_concatenated(data,label,subset_size):
    arr = np.arange(len(data))
    np.random.shuffle(arr)
    data_train = data[arr[0:subset_size]]
    label_train = label[arr[0:subset_size]]
    return data_train,label_train

labels.shape



Loss = []
Val_loss = []
epochs = 5000
learning_rate = 0.1
batch_size = 20

w = tf.Variable(np.random.random((2, 2)))
b = tf.Variable(np.random.random((2)))
data = np.column_stack((xs,ys))
data_train,label_train,data_val,label_val = split_dataset(data,labels,0.2)
for _ in range(epochs):

  data_batch,labels_batch = subset_dataset_concatenated(data_train,label_train,batch_size)
  data_val_batch,labels_val_batch = subset_dataset_concatenated(data_val,label_val,batch_size)

  with tf.GradientTape() as tape:

    pred_l=tf.nn.softmax(tf.matmul(data_batch, w) + b)
    pred_l_val=tf.nn.softmax(tf.matmul(data_val_batch, w) + b)

    labels_batch_one_hot = tf.one_hot(labels_batch, depth=2)
    labels_val_batch_one_hot = tf.one_hot(labels_val_batch, depth=2)


    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_batch_one_hot, pred_l)
    Loss.append(loss.numpy())

    val_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_val_batch_one_hot, pred_l_val)
    Val_loss.append(val_loss.numpy())
    print("loss",loss,"val_loss",val_loss)

  dloss_dw,dloss_db = tape.gradient(loss, [w, b])

  w.assign_sub(learning_rate*dloss_dw )
  b.assign_sub(learning_rate*dloss_db )

print(data_train.size,label_train.size,data_val.size,label_val.size)

np.max(Loss),np.min(Loss)

np.max(Val_loss),np.min(Val_loss)

print(w.numpy())
print(b.numpy())

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()



x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=4.0
y=5.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

"""#Hiperparametria

##Learning rate 0.01
"""



Loss = []
Val_loss = []
epochs = 5000
learning_rate = 0.01
batch_size = 20

w = tf.Variable(np.random.random((2, 2)))
b = tf.Variable(np.random.random((2)))
data = np.column_stack((xs,ys))
data_train,label_train,data_val,label_val = split_dataset(data,labels,0.2)
for _ in range(epochs):

  data_batch,labels_batch = subset_dataset_concatenated(data_train,label_train,batch_size)
  data_val_batch,labels_val_batch = subset_dataset_concatenated(data_val,label_val,batch_size)

  with tf.GradientTape() as tape:

    pred_l=tf.nn.softmax(tf.matmul(data_batch, w) + b)
    pred_l_val=tf.nn.softmax(tf.matmul(data_val_batch, w) + b)

    labels_batch_one_hot = tf.one_hot(labels_batch, depth=2)
    labels_val_batch_one_hot = tf.one_hot(labels_val_batch, depth=2)


    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_batch_one_hot, pred_l)
    Loss.append(loss.numpy())

    val_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_val_batch_one_hot, pred_l_val)
    Val_loss.append(val_loss.numpy())
    print("loss",loss,"val_loss",val_loss)

  dloss_dw,dloss_db = tape.gradient(loss, [w, b])

  w.assign_sub(learning_rate*dloss_dw )
  b.assign_sub(learning_rate*dloss_db )

print(data_train.size,label_train.size,data_val.size,label_val.size)

np.max(Loss),np.min(Loss)

np.max(Val_loss),np.min(Val_loss)

print(w.numpy())
print(b.numpy())

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=4.0
y=5.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

"""Learning rate 0.001"""



Loss = []
Val_loss = []
epochs = 5000
learning_rate = 0.1
batch_size = 20

w = tf.Variable(np.random.random((2, 2)))
b = tf.Variable(np.random.random((2)))
data = np.column_stack((xs,ys))
data_train,label_train,data_val,label_val = split_dataset(data,labels,0.2)
for _ in range(epochs):

  data_batch,labels_batch = subset_dataset_concatenated(data_train,label_train,batch_size)
  data_val_batch,labels_val_batch = subset_dataset_concatenated(data_val,label_val,batch_size)

  with tf.GradientTape() as tape:

    pred_l=tf.nn.softmax(tf.matmul(data_batch, w) + b)
    pred_l_val=tf.nn.softmax(tf.matmul(data_val_batch, w) + b)

    labels_batch_one_hot = tf.one_hot(labels_batch, depth=2)
    labels_val_batch_one_hot = tf.one_hot(labels_val_batch, depth=2)


    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_batch_one_hot, pred_l)
    Loss.append(loss.numpy())

    val_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_val_batch_one_hot, pred_l_val)
    Val_loss.append(val_loss.numpy())
    print("loss",loss,"val_loss",val_loss)

  dloss_dw,dloss_db = tape.gradient(loss, [w, b])

  w.assign_sub(learning_rate*dloss_dw )
  b.assign_sub(learning_rate*dloss_db )

print(data_train.size,label_train.size,data_val.size,label_val.size)

np.max(Loss),np.min(Loss)

np.max(Val_loss),np.min(Val_loss)

print(w.numpy())
print(b.numpy())

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=4.0
y=5.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

"""#Number of epchos - 100"""



Loss = []
Val_loss = []
epochs = 100
learning_rate = 0.1
batch_size = 20

w = tf.Variable(np.random.random((2, 2)))
b = tf.Variable(np.random.random((2)))
data = np.column_stack((xs,ys))
data_train,label_train,data_val,label_val = split_dataset(data,labels,0.2)
for _ in range(epochs):

  data_batch,labels_batch = subset_dataset_concatenated(data_train,label_train,batch_size)
  data_val_batch,labels_val_batch = subset_dataset_concatenated(data_val,label_val,batch_size)

  with tf.GradientTape() as tape:

    pred_l=tf.nn.softmax(tf.matmul(data_batch, w) + b)
    pred_l_val=tf.nn.softmax(tf.matmul(data_val_batch, w) + b)

    labels_batch_one_hot = tf.one_hot(labels_batch, depth=2)
    labels_val_batch_one_hot = tf.one_hot(labels_val_batch, depth=2)


    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_batch_one_hot, pred_l)
    Loss.append(loss.numpy())

    val_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_val_batch_one_hot, pred_l_val)
    Val_loss.append(val_loss.numpy())
    print("loss",loss,"val_loss",val_loss)

  dloss_dw,dloss_db = tape.gradient(loss, [w, b])

  w.assign_sub(learning_rate*dloss_dw )
  b.assign_sub(learning_rate*dloss_db )

print(data_train.size,label_train.size,data_val.size,label_val.size)

np.max(Loss),np.min(Loss)

np.max(Val_loss),np.min(Val_loss)

print(w.numpy())
print(b.numpy())

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=4.0
y=5.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

"""#Number of epochs - 3000"""



Loss = []
Val_loss = []
epochs = 3000
learning_rate = 0.1
batch_size = 20

w = tf.Variable(np.random.random((2, 2)))
b = tf.Variable(np.random.random((2)))
data = np.column_stack((xs,ys))
data_train,label_train,data_val,label_val = split_dataset(data,labels,0.2)
for _ in range(epochs):

  data_batch,labels_batch = subset_dataset_concatenated(data_train,label_train,batch_size)
  data_val_batch,labels_val_batch = subset_dataset_concatenated(data_val,label_val,batch_size)

  with tf.GradientTape() as tape:

    pred_l=tf.nn.softmax(tf.matmul(data_batch, w) + b)
    pred_l_val=tf.nn.softmax(tf.matmul(data_val_batch, w) + b)

    labels_batch_one_hot = tf.one_hot(labels_batch, depth=2)
    labels_val_batch_one_hot = tf.one_hot(labels_val_batch, depth=2)


    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_batch_one_hot, pred_l)
    Loss.append(loss.numpy())

    val_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_val_batch_one_hot, pred_l_val)
    Val_loss.append(val_loss.numpy())
    print("loss",loss,"val_loss",val_loss)

  dloss_dw,dloss_db = tape.gradient(loss, [w, b])

  w.assign_sub(learning_rate*dloss_dw )
  b.assign_sub(learning_rate*dloss_db )

print(data_train.size,label_train.size,data_val.size,label_val.size)

np.max(Loss),np.min(Loss)

np.max(Val_loss),np.min(Val_loss)

print(w.numpy())
print(b.numpy())

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=4.0
y=5.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

"""#Minibatch"""



"""#Batch size - 10"""



Loss = []
Val_loss = []
epochs = 5000
learning_rate = 0.1
batch_size = 10

w = tf.Variable(np.random.random((2, 2)))
b = tf.Variable(np.random.random((2)))
data = np.column_stack((xs,ys))
data_train,label_train,data_val,label_val = split_dataset(data,labels,0.2)
for _ in range(epochs):

  data_batch,labels_batch = subset_dataset_concatenated(data_train,label_train,batch_size)
  data_val_batch,labels_val_batch = subset_dataset_concatenated(data_val,label_val,batch_size)

  with tf.GradientTape() as tape:

    pred_l=tf.nn.softmax(tf.matmul(data_batch, w) + b)
    pred_l_val=tf.nn.softmax(tf.matmul(data_val_batch, w) + b)

    labels_batch_one_hot = tf.one_hot(labels_batch, depth=2)
    labels_val_batch_one_hot = tf.one_hot(labels_val_batch, depth=2)


    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_batch_one_hot, pred_l)
    Loss.append(loss.numpy())

    val_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_val_batch_one_hot, pred_l_val)
    Val_loss.append(val_loss.numpy())
    print("loss",loss,"val_loss",val_loss)

  dloss_dw,dloss_db = tape.gradient(loss, [w, b])

  w.assign_sub(learning_rate*dloss_dw )
  b.assign_sub(learning_rate*dloss_db )

print(data_train.size,label_train.size,data_val.size,label_val.size)

np.max(Loss),np.min(Loss)

np.max(Val_loss),np.min(Val_loss)

print(w.numpy())
print(b.numpy())

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=4.0
y=5.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

"""#Batch size - 100"""



Loss = []
Val_loss = []
epochs = 5000
learning_rate = 0.1
batch_size = 100

w = tf.Variable(np.random.random((2, 2)))
b = tf.Variable(np.random.random((2)))
data = np.column_stack((xs,ys))
data_train,label_train,data_val,label_val = split_dataset(data,labels,0.2)
for _ in range(epochs):

  data_batch,labels_batch = subset_dataset_concatenated(data_train,label_train,batch_size)
  data_val_batch,labels_val_batch = subset_dataset_concatenated(data_val,label_val,batch_size)

  with tf.GradientTape() as tape:

    pred_l=tf.nn.softmax(tf.matmul(data_batch, w) + b)
    pred_l_val=tf.nn.softmax(tf.matmul(data_val_batch, w) + b)

    labels_batch_one_hot = tf.one_hot(labels_batch, depth=2)
    labels_val_batch_one_hot = tf.one_hot(labels_val_batch, depth=2)


    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_batch_one_hot, pred_l)
    Loss.append(loss.numpy())

    val_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels_val_batch_one_hot, pred_l_val)
    Val_loss.append(val_loss.numpy())
    print("loss",loss,"val_loss",val_loss)

  dloss_dw,dloss_db = tape.gradient(loss, [w, b])

  w.assign_sub(learning_rate*dloss_dw )
  b.assign_sub(learning_rate*dloss_db )

print(data_train.size,label_train.size,data_val.size,label_val.size)

np.max(Loss),np.min(Loss)

np.max(Val_loss),np.min(Val_loss)

print(w.numpy())
print(b.numpy())

plt.plot(Loss)
plt.plot(Val_loss)
plt.show()

x=3.0
y=2.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=7.0
y=6.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=4.0
y=5.0
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

"""Najlepsze wyniki otrzymałem dla współczynnika uczenia 0.1, liczby epok 5000, batcha równego 20, najgorsze dla współczynnika uczenia 0.001, liczby epok 100, batcha równego 100.

I got the best results for a learning rate of 0.1, a number of epochs of 5000, a batch of 20, and the worst for a learning rate of 0.001, a number of epochs of 100, a batch of 100.
"""



