# -*- coding: utf-8 -*-
"""SAI_lab_11_MNIST_Fasion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CaoKvrWIE4GQFTA_WgPXSSSvx13XvQjo

# CNN na przykładzie MNIST

# Setup
Importujemy potrzebne biblioteki
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import keras

"""# 1 Przygotowanie danych

## 1.0 Pobranie zbioru danych
Pobieramy zbiór danych i sprawdzamy rozmiar 28 x 28 pixeli.
"""

(x_train_data, y_train_data), (x_test_data, y_test_data) = tf.keras.datasets.fashion_mnist.load_data()

dataset_labels = ["0",  # index 0
                  "1",  # index 1
                  "2",  # index 2
                  "3",  # index 3
                  "4",  # index 4
                  "5",  # index 5
                  "6",  # index 6
                  "7",  # index 7
                  "8",  # index 8
                  "9"]  # index 9

print("x_train shape:", x_train_data.shape, "y_train shape:", y_train_data.shape)
print("x_test shape:", x_test_data.shape, "y_test shape:", y_test_data.shape)

"""## 1.1 Wizualizacja danych
Przykładowy obrazek ze bioru danych
"""

def plot_image(img_index):
    label_index = y_train_data[img_index]
    plt.imshow(x_train_data[img_index]/255, cmap = 'gray')
    plt.title("Image "+str(img_index)+":"+dataset_labels[label_index])

img_index = 60
plot_image(img_index)

"""## 1.2 Normalizacja danych
Na początek sprawdzamy jakie są max i min wartości pixeli w obrazkach.

Wartości te powinny być zawarte w przedziale [0,1].
"""

print("Wartości min:",np.min(x_train_data)," max:",np.max(x_train_data))

x_train_data = x_train_data.astype('float32') / 255
x_test_data = x_test_data.astype('float32') / 255

print("Wartości po przeskalowaniu min:",np.min(x_train_data)," max:",np.max(x_train_data))

"""## 1.3 Podział zbioru danych na zbiór treningowy/walidacyjny/testowy
*   **Zbiór treningowy** - wykorzystamy go do uczenia.
*   **Zbiór walidacyjny** - wykorzystamy go do tuningu hiperparametrów.
*   **Zbiór testowy** - wykorzystamy go do ostatecznego sprawdzenia modelu.

Zbiór walidacyjny stworzymy z 10% zbioru treningowego.
"""

validation_fraction = .1

total_train_samples = len(x_train_data)
validation_samples = int(total_train_samples * validation_fraction)
train_samples = total_train_samples - validation_samples

(x_train, x_valid) = x_train_data[:train_samples], x_train_data[train_samples:]
(y_train, y_valid) = y_train_data[:train_samples], y_train_data[train_samples:]

x_test, y_test = x_test_data, y_test_data
print(train_samples, validation_samples, len(x_test))

"""## 1.4 Dwa dodatkowe kroki
1. Większość zestawów danych obrazu składa się z obrazów rgb. Z tego powodu Keras oczekuje, że każdy obraz będzie miał 3 wymiary: [x_pixels, y_pixels, color_channels]. Ponieważ nasze obrazki są w skali szarości, wymiar koloru jest równy 1. Musimy zatem zmienić kształt obrazków.

2. W procesie uczenia naszego modelu będziemy wykorzystwali tzw. **kategoryczną entropię krzyżową** (https://keras.io/losses/). Musimy przekształcić wektory z etykietami (labelami) do **kodowania one-hot**. Wykorzystamy do tego funkcję tf.keras.utils.to_categorical().
"""

# Zmieniamy kształ z (28, 28) na (28, 28, 1)
w, h = 28, 28
x_train = x_train.reshape(x_train.shape[0], w, h, 1)
x_valid = x_valid.reshape(x_valid.shape[0], w, h, 1)
x_test = x_test.reshape(x_test.shape[0], w, h, 1)

# Kodowanie one-hot
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_valid = tf.keras.utils.to_categorical(y_valid, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

print("x_train shape:", x_train.shape, "y_train shape:", y_train.shape)

# Ilość elmentów w zbiorach
print(x_train.shape[0], 'train set')
print(x_valid.shape[0], 'validation set')
print(x_test.shape[0], 'test set')

"""# 2 Stworzenie modelu

Keras oferuje dwa API:
1. [Sequential model API](https://keras.io/models/sequential/)
2. [Functional API](https://keras.io/models/model/)

W naszym modelu wykorzystamy Sequential model API. Będziemy wykorzystwali następujące metody:

*   Dense()    [link text](https://keras.io/layers/core/) - tworzy **warstwę gęstą**
*   Conv2D()   [link text](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D/) - tworzy **warstwę konwolucyjną**
*   Pooling()  [link text](https://keras.io/layers/pooling/) - tworzy **warstwę pooling**
*   Dropout()  [link text](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) - zastowanie **dropout**

## 2.0 Prosty model liniowy
Zaczniemy od prostego modelu składającego się z jednej transformacji liniowej.

*  Model stworzymy za pomocą tf.keras.Sequential() (https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential).
Ponieważ nie zastosujemy jeszcze konwolucji zatem możemy spłaszczyć obrazki do wektorów zawierających 28x28 wartości.

*  Następnie dodamy jedną warstwę liniową, która przkształci wejściowe piksele w 10 klas. Poniważ wyniki reprezentują prawdopodobieństwa możemy użyć funkcji aktywacji softmax.

*  Szczegóły modelu uzyskamy z pomocą model.summary()
"""

model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))
model.add(tf.keras.layers.Dense(10,activation="softmax"))
model.summary()

"""## Kompilacja modelu
Uwagi:

*   Użyjemy **optymizera adam**
*   Jako loss function użyjemy '**categorical_crossentropy**'
*   Lista parametrów, tutaj zaczniemy od '**precyzji**'

Warto zerknąć: https://keras.io/models/model/
"""

model.compile(optimizer="adam", loss='categorical_crossentropy', metrics=['accuracy']) #learnig rate???

"""## Uczenie modelu

Model uczymy wykorzystując fit().
"""

history = model.fit(x_train, y_train, batch_size = 64, epochs = 5, validation_data = (x_valid,y_valid))

"""Wykresy precyzji i błędu"""

import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

"""## Zapisanie i wczytanie modelu

Zapisanie modelu
"""

model.save("mnist_simple.h5")

"""Wczytanie modelu"""

#from keras.models import load_model
#model = load_model("mnist_simple.h5")

"""## Precyzja
Wykorzystamy funkcję evaluate()
"""

score = model.evaluate(x_test,y_test,verbose=0)
print('Test accuracy:',score[1])

"""## Przewidywania modelu
Przetestujmy przewidywania naszego modelu. Sprawdzimy go na danych testowych. W tym celu wykorzystamy poniższą funkcję 'visualize_model_predictions(model, x, y)'
"""

def visualize_model_predictions(model, x_test, y_test, title_string):
    y_hat = model.predict(x_test)

    figure = plt.figure(figsize=(20, 8))
    for i, index in enumerate(np.random.choice(x_test.shape[0], size=32, replace=False)):
        ax = figure.add_subplot(4, 8, i + 1, xticks=[], yticks=[])

        ax.imshow(np.squeeze(x_test[index]), cmap = 'gray')
        predict_index = np.argmax(y_hat[index])
        true_index = np.argmax(y_test[index])

        ax.set_title("{} ({})".format(dataset_labels[predict_index],
                                      dataset_labels[true_index]),
                                      color=("green" if predict_index == true_index else "red"))
    figure.suptitle("%s wyniki:" %title_string, fontsize=25)

visualize_model_predictions(model, x_test, y_test, 'Test')

"""## 1. Wizualizacja wag dla każdej klasy
Warstwę transformacyjną naszgo modelu można przedstawić za pomocą macierzy wag [28x28, 10]. Spróbujmy narysować każdy z 10 filtrów. W celu uzyskania wag modelu wykorzystamy funkcje model.layers i get_weights().


"""

for layer in model.layers:
    weights = layer.get_weights()
    if len(weights) > 0:
        w,b = weights
        filters = np.reshape(w, (28,28,10))

def visualize_filters(filters, title_string):
    figure = plt.figure(figsize=(20, 8))
    for i, index in enumerate(np.random.choice(x_test.shape[0], size=10, replace=False)):
        ax = figure.add_subplot(2, 5, i + 1, xticks=[], yticks=[])
        ax.imshow(filters[:,:,i], cmap = 'viridis')
        ax.set_title("%s dla %s" %(title_string, dataset_labels[i]))

visualize_filters(filters, 'Filtr')

"""## 2 I jeszcze jedna wizualizacja
Porównajmy powyższe filtry, ze średnim zdjęciem dla każdej klasy.
"""

avg_images = np.zeros((28,28,1,10))
class_images = [0]*10

for i in range(len(x_train)):
    img = x_train[i]
    label = np.argmax(y_train[i])

    avg_images[:,:,:,label] += img
    class_images[label] += 1

for i in range(10):
    avg_images[:,:,:,i] = avg_images[:,:,:,i]/class_images[i]

avg_images = np.squeeze(avg_images)
visualize_filters(avg_images, 'Średni obrazek dla')



"""## 2.0 Prosty model liniowy v2
Zaczniemy od prostego modelu składającego się z jednej transformacji liniowej.

*  Model stworzymy za pomocą tf.keras.Sequential() (https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential).
Ponieważ nie zastosujemy jeszcze konwolucji zatem możemy spłaszczyć obrazki do wektorów zawierających 28x28 wartości.

*  Następnie dodamy jedną warstwę liniową, która przkształci wejściowe piksele w 10 klas. Poniważ wyniki reprezentują prawdopodobieństwa możemy użyć funkcji aktywacji softmax.

*  Szczegóły modelu uzyskamy z pomocą model.summary()

"""

model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))
model.add(tf.keras.layers.Dense(10,activation="softmax"))
model.summary()

"""## Kompilacja modelu
Uwagi:

*   Użyjemy **optymizera adam**
*   Jako loss function użyjemy '**categorical_crossentropy**'
*   Lista parametrów, tutaj zaczniemy od '**precyzji**'

Warto zerknąć: https://keras.io/models/model/
"""

opt = keras.optimizers.Adam(learning_rate=0.002)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) #learnig rate???

"""## Uczenie modelu

Model uczymy wykorzystując fit().
"""

history = model.fit(x_train, y_train, batch_size = 32, epochs = 5, validation_data = (x_valid,y_valid))

"""Wykresy precyzji i błędu"""

import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

"""## Zapisanie i wczytanie modelu

Zapisanie modelu
"""

model.save("mnist_simple.h5")

"""Wczytanie modelu"""

#from keras.models import load_model
#model = load_model("mnist_simple.h5")

"""## Precyzja
Wykorzystamy funkcję evaluate()
"""

score = model.evaluate(x_test,y_test,verbose=0)
print('Test accuracy:',score[1])

"""## Przewidywania modelu
Przetestujmy przewidywania naszego modelu. Sprawdzimy go na danych testowych. W tym celu wykorzystamy poniższą funkcję 'visualize_model_predictions(model, x, y)'
"""

def visualize_model_predictions(model, x_test, y_test, title_string):
    y_hat = model.predict(x_test)

    figure = plt.figure(figsize=(20, 8))
    for i, index in enumerate(np.random.choice(x_test.shape[0], size=32, replace=False)):
        ax = figure.add_subplot(4, 8, i + 1, xticks=[], yticks=[])

        ax.imshow(np.squeeze(x_test[index]), cmap = 'gray')
        predict_index = np.argmax(y_hat[index])
        true_index = np.argmax(y_test[index])

        ax.set_title("{} ({})".format(dataset_labels[predict_index],
                                      dataset_labels[true_index]),
                                      color=("green" if predict_index == true_index else "red"))
    figure.suptitle("%s wyniki:" %title_string, fontsize=25)

visualize_model_predictions(model, x_test, y_test, 'Test')

"""## 1. Wizualizacja wag dla każdej klasy
Warstwę transformacyjną naszgo modelu można przedstawić za pomocą macierzy wag [28x28, 10]. Spróbujmy narysować każdy z 10 filtrów. W celu uzyskania wag modelu wykorzystamy funkcje model.layers i get_weights().


"""

for layer in model.layers:
    weights = layer.get_weights()
    if len(weights) > 0:
        w,b = weights
        filters = np.reshape(w, (28,28,10))

def visualize_filters(filters, title_string):
    figure = plt.figure(figsize=(20, 8))
    for i, index in enumerate(np.random.choice(x_test.shape[0], size=10, replace=False)):
        ax = figure.add_subplot(2, 5, i + 1, xticks=[], yticks=[])
        ax.imshow(filters[:,:,i], cmap = 'viridis')
        ax.set_title("%s dla %s" %(title_string, dataset_labels[i]))

visualize_filters(filters, 'Filtr')

"""## 2 I jeszcze jedna wizualizacja
Porównajmy powyższe filtry, ze średnim zdjęciem dla każdej klasy.
"""

avg_images = np.zeros((28,28,1,10))
class_images = [0]*10

for i in range(len(x_train)):
    img = x_train[i]
    label = np.argmax(y_train[i])

    avg_images[:,:,:,label] += img
    class_images[label] += 1

for i in range(10):
    avg_images[:,:,:,i] = avg_images[:,:,:,i]/class_images[i]

avg_images = np.squeeze(avg_images)
visualize_filters(avg_images, 'Średni obrazek dla')



"""## 2.0 Prosty model liniowy v3
Zaczniemy od prostego modelu składającego się z jednej transformacji liniowej.

*  Model stworzymy za pomocą tf.keras.Sequential() (https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential).
Ponieważ nie zastosujemy jeszcze konwolucji zatem możemy spłaszczyć obrazki do wektorów zawierających 28x28 wartości.

*  Następnie dodamy jedną warstwę liniową, która przkształci wejściowe piksele w 10 klas. Poniważ wyniki reprezentują prawdopodobieństwa możemy użyć funkcji aktywacji softmax.

*  Szczegóły modelu uzyskamy z pomocą model.summary()

"""

model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))
model.add(tf.keras.layers.Dense(10,activation="softmax"))
model.summary()

"""## Kompilacja modelu
Uwagi:

*   Użyjemy **optymizera sgd**
*   Jako loss function użyjemy '**categorical_crossentropy**'
*   Lista parametrów, tutaj zaczniemy od '**precyzji**'

Warto zerknąć: https://keras.io/models/model/
"""

opt = keras.optimizers.SGD(learning_rate=0.04)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

"""## Uczenie modelu

Model uczymy wykorzystując fit().
"""

history = model.fit(x_train, y_train, batch_size = 64, epochs = 75, validation_data = (x_valid,y_valid))

"""Wykresy precyzji i błędu"""

import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

"""## Zapisanie i wczytanie modelu

Zapisanie modelu
"""

model.save("mnist_simple.h5")

"""Wczytanie modelu"""

#from keras.models import load_model
#model = load_model("mnist_simple.h5")

"""## Precyzja
Wykorzystamy funkcję evaluate()
"""

score = model.evaluate(x_test,y_test,verbose=0)
print('Test accuracy:',score[1])

"""## Przewidywania modelu
Przetestujmy przewidywania naszego modelu. Sprawdzimy go na danych testowych. W tym celu wykorzystamy poniższą funkcję 'visualize_model_predictions(model, x, y)'
"""

def visualize_model_predictions(model, x_test, y_test, title_string):
    y_hat = model.predict(x_test)

    figure = plt.figure(figsize=(20, 8))
    for i, index in enumerate(np.random.choice(x_test.shape[0], size=32, replace=False)):
        ax = figure.add_subplot(4, 8, i + 1, xticks=[], yticks=[])

        ax.imshow(np.squeeze(x_test[index]), cmap = 'gray')
        predict_index = np.argmax(y_hat[index])
        true_index = np.argmax(y_test[index])

        ax.set_title("{} ({})".format(dataset_labels[predict_index],
                                      dataset_labels[true_index]),
                                      color=("green" if predict_index == true_index else "red"))
    figure.suptitle("%s wyniki:" %title_string, fontsize=25)

visualize_model_predictions(model, x_test, y_test, 'Test')

"""## 1. Wizualizacja wag dla każdej klasy
Warstwę transformacyjną naszgo modelu można przedstawić za pomocą macierzy wag [28x28, 10]. Spróbujmy narysować każdy z 10 filtrów. W celu uzyskania wag modelu wykorzystamy funkcje model.layers i get_weights().


"""

for layer in model.layers:
    weights = layer.get_weights()
    if len(weights) > 0:
        w,b = weights
        filters = np.reshape(w, (28,28,10))

def visualize_filters(filters, title_string):
    figure = plt.figure(figsize=(20, 8))
    for i, index in enumerate(np.random.choice(x_test.shape[0], size=10, replace=False)):
        ax = figure.add_subplot(2, 5, i + 1, xticks=[], yticks=[])
        ax.imshow(filters[:,:,i], cmap = 'viridis')
        ax.set_title("%s dla %s" %(title_string, dataset_labels[i]))

visualize_filters(filters, 'Filtr')

"""## 2 I jeszcze jedna wizualizacja
Porównajmy powyższe filtry, ze średnim zdjęciem dla każdej klasy.
"""

avg_images = np.zeros((28,28,1,10))
class_images = [0]*10

for i in range(len(x_train)):
    img = x_train[i]
    label = np.argmax(y_train[i])

    avg_images[:,:,:,label] += img
    class_images[label] += 1

for i in range(10):
    avg_images[:,:,:,i] = avg_images[:,:,:,i]/class_images[i]

avg_images = np.squeeze(avg_images)
visualize_filters(avg_images, 'Średni obrazek dla')

"""# 2.1 A teraz sieć neuronowa
Dodajmy teraz warstwy wewnętrzne w naszej sieci. Funkcja aktywacji w takich warstwach to zwykle relu.
"""

model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))
model.add(tf.keras.layers.Dense(60,activation="relu"))
model.add(tf.keras.layers.Dense(10,activation="softmax"))

model.summary()

model.compile(optimizer="adam", loss='categorical_crossentropy', metrics=['accuracy']) #learnig rate???

history = model.fit(x_train, y_train, batch_size = 64, epochs = 10, validation_data = (x_valid,y_valid))

"""Precyzja"""

score = model.evaluate(x_test,y_test,verbose=0)
print('Precyzja: ',score[1])

"""Wykresy precyzji i błędu"""

import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

"""## Przewidywania modelu
Sprawdźmy jakie są przewidywania naszego modelu
"""

visualize_model_predictions(model, x_test, y_test, "test" )



"""# 2.1 Sieć neuronowa v2
Dodajmy teraz warstwy wewnętrzne w naszej sieci. Funkcja aktywacji w takich warstwach to zwykle relu.
"""

model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))
model.add(tf.keras.layers.Dense(64,activation="relu"))
model.add(tf.keras.layers.Dense(10,activation="softmax"))

model.summary()
opt = keras.optimizers.Adam(learning_rate=0.004)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) #learnig rate???

history = model.fit(x_train, y_train, batch_size = 64, epochs = 13, validation_data = (x_valid,y_valid))

"""Precyzja"""

score = model.evaluate(x_test,y_test,verbose=0)
print('Precyzja: ',score[1])

"""Wykresy precyzji i błędu"""

import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

"""## Przewidywania modelu
Sprawdźmy jakie są przewidywania naszego modelu
"""

visualize_model_predictions(model, x_test, y_test, "test" )



"""# 2.1 Sieć neuronowa v3

Dodajmy teraz warstwy wewnętrzne w naszej sieci. Funkcja aktywacji w takich warstwach to zwykle relu.
"""

model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))
model.add(tf.keras.layers.Dense(64,activation="relu"))
model.add(tf.keras.layers.Dense(10,activation="softmax"))

model.summary()
opt = keras.optimizers.SGD(learning_rate=0.08)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size = 128, epochs = 50, validation_data = (x_valid,y_valid))

"""Precyzja"""

score = model.evaluate(x_test,y_test,verbose=0)
print('Precyzja: ',score[1])

"""Wykresy precyzji i błędu"""

import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

"""## Przewidywania modelu
Sprawdźmy jakie są przewidywania naszego modelu
"""

visualize_model_predictions(model, x_test, y_test, "test" )

"""## 2.2 Spróbujmy pogłębić nasz model!
Dodajmy 3 warstwy gęste.
"""

model = tf.keras.Sequential()

model.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(32, activation='relu'))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size = 64, epochs = 10, validation_data = (x_valid,y_valid))

score = model.evaluate(x_test,y_test,verbose=0)
print('Precyzja: ',score[1])

"""Wykresy precyzji i błędu"""

import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

visualize_model_predictions(model, x_test, y_test, "test" )



"""## 2.2 Pogłębinienie modelu v2
Dodajmy 2 warstwy gęste.
"""

model = tf.keras.Sequential()

model.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(32, activation='relu'))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size = 64, epochs = 10, validation_data = (x_valid,y_valid))

score = model.evaluate(x_test,y_test,verbose=0)
print('Precyzja: ',score[1])

"""Wykresy precyzji i błędu"""

import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

visualize_model_predictions(model, x_test, y_test, "test" )



"""## 2.2 Pogłębiamy model v3
Dodajmy 4 warstwy gęste.
"""

model = tf.keras.Sequential()

model.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(32, activation='relu'))
model.add(tf.keras.layers.Dense(10, activation='softmax'))

model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size = 64, epochs = 10, validation_data = (x_valid,y_valid))

score = model.evaluate(x_test,y_test,verbose=0)
print('Precyzja: ',score[1])

"""Wykresy precyzji i błędu"""

import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

visualize_model_predictions(model, x_test, y_test, "test" )

"""## 2.3 Konwolucja

W dotychczasowych przykładach przed warstwą gęstą dodawaliśmy warstę płaską. Tutaj będzie podobnie, ale przed warstwą płaską dodamy warstwy konwolucyjne i maxpool.
"""

model = tf.keras.Sequential()

model.add(tf.keras.layers.Conv2D(filters=4, kernel_size=2,padding='same',activation='relu',input_shape=(28,28,1)))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

model.add(tf.keras.layers.Conv2D(filters=2, kernel_size=2,padding='same',activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(10,activation="softmax"))

model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=128, epochs=25, validation_data=(x_valid, y_valid))

"""## Ewaluacja modelu"""

score = model.evaluate(x_test, y_test, verbose=0)
print('Precyzja: ',score[1])

"""Wykresy precyzji i błędu"""

import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

visualize_model_predictions(model, x_test, y_test,"convnet")





"""## 2.3 Konwolucja model v2
W dotychczasowych przykładach przed warstwą gęstą dodawaliśmy warstę płaską. Tutaj będzie podobnie, ale przed warstwą płaską dodamy warstwy konwolucyjne i maxpool.
"""

model = tf.keras.Sequential()

model.add(tf.keras.layers.Conv2D(filters=4, kernel_size=2,padding='same',activation='relu',input_shape=(28,28,1)))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

model.add(tf.keras.layers.Conv2D(filters=2, kernel_size=2,padding='same',activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

model.add(tf.keras.layers.Conv2D(filters=2, kernel_size=2,padding='same',activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(10,activation="softmax"))

model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=128, epochs=25, validation_data=(x_valid, y_valid))

"""## Ewaluacja modelu"""

score = model.evaluate(x_test, y_test, verbose=0)
print('Precyzja: ',score[1])

"""Wykresy precyzji i błędu"""

import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

visualize_model_predictions(model, x_test, y_test,"convnet")



"""## 2.3 Konwolucja model v3
W dotychczasowych przykładach przed warstwą gęstą dodawaliśmy warstę płaską. Tutaj będzie podobnie, ale przed warstwą płaską dodamy warstwy konwolucyjne i maxpool.
"""

model = tf.keras.Sequential()

model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2,padding='same',activation='relu',input_shape=(28,28,1)))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,padding='same',activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2,padding='same',activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))


model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(10,activation="softmax"))

model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=128, epochs=25, validation_data=(x_valid, y_valid))

"""## Ewaluacja modelu"""

score = model.evaluate(x_test, y_test, verbose=0)
print('Precyzja: ',score[1])

"""Wykresy precyzji i błędu"""

import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

visualize_model_predictions(model, x_test, y_test,"convnet")

"""# 3 Regularyzacja
Nasz model ma obecnie dużo stopni swobody (ma DUŻO parametrów i dlatego może dopasować się do niemal każdej funkcji, jeśli tylko będziemy trenować wystarczająco długo). Oznacza to, że nasza sieć jest również podatna na przeuczenie.

W tej sekcji dodajmy warstwy dropout pomiędzy głównymi warstwami naszej sieci, aby uniknąć przeuczenia.
"""

model = tf.keras.Sequential()

model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2,padding='same',activation='relu',input_shape=(28,28,1)))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

model.add(tf.keras.layers.Dropout(0.5))

model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,padding='same',activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

model.add(tf.keras.layers.Dropout(0.5))

model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2,padding='same',activation='relu'))

model.add(tf.keras.layers.Flatten())

model.add(tf.keras.layers.Dropout(0.5))

model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(10,activation="softmax"))

model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x_train,
          y_train,
          batch_size=128,
          epochs=25,
          validation_data=(x_valid, y_valid))

"""## Evaluate model:"""

test_score = model.evaluate(x_test, y_test, verbose=0)
train_score = model.evaluate(x_train, y_train, verbose=0)

print('Train accuracy: ',train_score[1],' Test accuracy: ',test_score[1])

"""Wykresy precyzji i błędu"""

import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

visualize_model_predictions(model, x_test, y_test,"convnet")



"""# 3 Regularyzacja v2
Nasz model ma obecnie dużo stopni swobody (ma DUŻO parametrów i dlatego może dopasować się do niemal każdej funkcji, jeśli tylko będziemy trenować wystarczająco długo). Oznacza to, że nasza sieć jest również podatna na przeuczenie.

W tej sekcji dodajmy warstwy dropout pomiędzy warstwami naszej sieci, aby uniknąć przeuczenia.
"""

model = tf.keras.Sequential()

model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2,padding='same',activation='relu',input_shape=(28,28,1)))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

model.add(tf.keras.layers.Dropout(0.5))

model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,padding='same',activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

model.add(tf.keras.layers.Dropout(0.5))

model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2,padding='same',activation='relu'))

model.add(tf.keras.layers.Flatten())

model.add(tf.keras.layers.Dropout(0.5))

model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.1))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(10,activation="softmax"))

model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x_train,
          y_train,
          batch_size=128,
          epochs=25,
          validation_data=(x_valid, y_valid))

"""## Evaluate model:"""

test_score = model.evaluate(x_test, y_test, verbose=0)
train_score = model.evaluate(x_train, y_train, verbose=0)

print('Train accuracy: ',train_score[1],' Test accuracy: ',test_score[1])

"""Wykresy precyzji i błędu"""

import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

visualize_model_predictions(model, x_test, y_test,"convnet")



"""# 3 Regularyzacja v3
Nasz model ma obecnie dużo stopni swobody (ma DUŻO parametrów i dlatego może dopasować się do niemal każdej funkcji, jeśli tylko będziemy trenować wystarczająco długo). Oznacza to, że nasza sieć jest również podatna na przeuczenie.

W tej sekcji dodajmy warstwy dropout pomiędzy warstwami naszej sieci, aby uniknąć przeuczenia.
"""

model = tf.keras.Sequential()

model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2,padding='same',activation='relu',input_shape=(28,28,1)))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

model.add(tf.keras.layers.Dropout(0.4))

model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,padding='same',activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

model.add(tf.keras.layers.Dropout(0.4))

model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2,padding='same',activation='relu'))

model.add(tf.keras.layers.Flatten())

model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(256, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(10,activation="softmax"))

model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x_train,
          y_train,
          batch_size=128,
          epochs=60,
          validation_data=(x_valid, y_valid))

"""## Evaluate model:"""

test_score = model.evaluate(x_test, y_test, verbose=0)
train_score = model.evaluate(x_train, y_train, verbose=0)

print('Train accuracy: ',train_score[1],' Test accuracy: ',test_score[1])

import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

visualize_model_predictions(model, x_test, y_test,"convnet")

"""Wnioski

Najlepsze rezultaty uzyskuje model który stosuje konwolucję oraz regularyzacje w postaci warstw porzucających, ponieważ unika on wtedy przeuczenia. Najlepszym modelem jest model powyżej o nazwie Regularyzacja v3


```
model = tf.keras.Sequential()

model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2,padding='same',activation='relu',input_shape=(28,28,1)))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

model.add(tf.keras.layers.Dropout(0.4))

model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,padding='same',activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

model.add(tf.keras.layers.Dropout(0.4))

model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=2,padding='same',activation='relu'))

model.add(tf.keras.layers.Flatten())

model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(256, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(10,activation="softmax"))

model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
```



```
model.fit(x_train,
          y_train,
          batch_size=128,
          epochs=60,
          validation_data=(x_valid, y_valid))
```
"""

