# -*- coding: utf-8 -*-
"""SAI_lab_10_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jjlx7otWe0l5SrZeJp6qu5jHEUHhbff_
"""



"""Import biblioteki **TensorFlow** ([https://www.tensorflow.org/](https://www.tensorflow.org/)) z której będziemy korzystali w **uczeniu maszynowym**:"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

import keras
from keras.models import Sequential
from keras.layers import Dense

"""##Numbers recognition - dataset **MNIST**

Download dataset
"""

(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.mnist.load_data()



data = np.concatenate([train_data, test_data])

data.shape

label = np.concatenate([train_labels,test_labels])

label.shape

"""Informations about dataset"""

train_data.shape,train_labels.shape

test_data.shape,test_labels.shape

train_data[0]

train_labels[0]

"""One-hot encoding"""

train_labels = tf.keras.utils.to_categorical(train_labels, 10)
test_labels = tf.keras.utils.to_categorical(test_labels, 10)

train_data.shape,train_labels.shape

test_data.shape,test_labels.shape

train_labels[0]

"""Visulization"""

def plot_image(img_index):
    label_index = train_labels[img_index]
    plt.imshow(train_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

train_images = train_data.reshape((-1, 784))
test_images = test_data.reshape((-1, 784))

model = Sequential()
model.add(Dense(units = 128, use_bias=True, input_shape=(784,), activation = "relu"))
model.add(Dense(units = 10, use_bias=True, activation = "softmax"))

opt = keras.optimizers.Adam(learning_rate=0.001)
#opt = keras.optimizers.SGD(learning_rate=0.001)

model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])
model.summary()

batch_size = 128
epochs = 50

h = model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs,validation_split=0.2)

plt.scatter(np.arange(epochs),h.history['loss'])
plt.scatter(np.arange(epochs),h.history['val_loss'],c='r')
plt.show()

plt.scatter(np.arange(epochs),h.history['accuracy'])
plt.scatter(np.arange(epochs),h.history['val_accuracy'],c='y')
plt.show()

score = model.evaluate(test_images, test_labels, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

def plot_image(img_index):
    label_index = train_labels[img_index]
    plt.imshow(train_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

picture = train_data[img_index].reshape(-1,784)

model.predict(picture)

"""Import biblioteki **TensorFlow** ([https://www.tensorflow.org/](https://www.tensorflow.org/)) z której będziemy korzystali w **uczeniu maszynowym**:"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

import keras
from keras.models import Sequential
from keras.layers import Dense

"""##Numbers recognition - dataset **MNIST**

Download dataset
"""

(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.mnist.load_data()



(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.mnist.load_data()

data = np.concatenate([train_data, test_data])

data.shape

label = np.concatenate([train_labels,test_labels])

label.shape

"""Informations about dataset"""

train_data.shape,train_labels.shape

test_data.shape,test_labels.shape

train_data[0]

train_labels[0]

"""One-hot encoding"""

train_labels = tf.keras.utils.to_categorical(train_labels, 10)
test_labels = tf.keras.utils.to_categorical(test_labels, 10)

train_data.shape,train_labels.shape

test_data.shape,test_labels.shape

train_labels[0]

"""Visulization"""

def plot_image(img_index):
    label_index = test_labels[img_index]
    plt.imshow(test_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

train_images = train_data.reshape((-1, 784))
test_images = test_data.reshape((-1, 784))

model = Sequential()
model.add(Dense(units = 128, use_bias=True, input_shape=(784,), activation = "relu"))
model.add(Dense(units = 10, use_bias=True, activation = "softmax"))

opt = keras.optimizers.Adam(learning_rate=0.001)
#opt = keras.optimizers.SGD(learning_rate=0.001)

model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])
model.summary()

batch_size = 128
epochs = 50

h = model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_split=0.2)

plt.scatter(np.arange(epochs),h.history['loss'])
plt.scatter(np.arange(epochs),h.history['val_loss'],c='r')
plt.show()

plt.scatter(np.arange(epochs),h.history['accuracy'])
plt.scatter(np.arange(epochs),h.history['val_accuracy'],c='y')
plt.show()

score = model.evaluate(test_images, test_labels, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

def plot_image(img_index):
    label_index = test_labels[img_index]
    plt.imshow(test_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

picture = test_data[img_index].reshape(-1,784)

model.predict(picture)

"""# **Regularyzacja** - metoda 1

Zwiększamy zbiór treningowy z **60000** do **65000** (20% to zbiór walidacyjny)
"""



test_data = data[:65000]
train_data = data[65000:]
test_labels = label[:65000]
train_labels = label[65000:]

print(test_data.shape,train_data.shape,test_labels.shape,train_labels.shape)

"""One-hot coding"""

train_labels = tf.keras.utils.to_categorical(train_labels, 10)
test_labels = tf.keras.utils.to_categorical(test_labels, 10)

train_data.shape,train_labels.shape

test_data.shape,test_labels.shape

train_labels[0]



"""Visulization"""

def plot_image(img_index):
    label_index = test_labels[img_index]
    plt.imshow(test_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

train_images = train_data.reshape((-1, 784))
test_images = test_data.reshape((-1, 784))

model = Sequential()
model.add(Dense(units = 128, use_bias=True, input_shape=(784,), activation = "relu"))
model.add(Dense(units = 10, use_bias=True, activation = "softmax"))

opt = keras.optimizers.Adam(learning_rate=0.001)
#opt = keras.optimizers.SGD(learning_rate=0.001)

model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])
model.summary()

batch_size = 128
epochs = 50

h = model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_split=0.2)

plt.scatter(np.arange(epochs),h.history['loss'])
plt.scatter(np.arange(epochs),h.history['val_loss'],c='r')
plt.show()

plt.scatter(np.arange(epochs),h.history['accuracy'])
plt.scatter(np.arange(epochs),h.history['val_accuracy'],c='y')
plt.show()

score = model.evaluate(test_images, test_labels, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

def plot_image(img_index):
    label_index = test_labels[img_index]
    plt.imshow(test_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

picture = test_data[img_index].reshape(-1,784)

model.predict(picture)





"""Zwiększamy zbiór treningowy z **60000** do **68000** (20% to zbiór walidacyjny)


"""

test_data = data[:68000]
train_data = data[68000:]
test_labels = label[:68000]
train_labels = label[68000:]

print(test_data.shape,train_data.shape,test_labels.shape,train_labels.shape)

"""One-hot coding"""

train_labels = tf.keras.utils.to_categorical(train_labels, 10)
test_labels = tf.keras.utils.to_categorical(test_labels, 10)

train_data.shape,train_labels.shape

test_data.shape,test_labels.shape

train_labels[0]

"""Visulization"""

def plot_image(img_index):
    label_index = test_labels[img_index]
    plt.imshow(test_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

train_images = train_data.reshape((-1, 784))
test_images = test_data.reshape((-1, 784))

model = Sequential()
model.add(Dense(units = 128, use_bias=True, input_shape=(784,), activation = "relu"))
model.add(Dense(units = 10, use_bias=True, activation = "softmax"))

opt = keras.optimizers.Adam(learning_rate=0.001)
#opt = keras.optimizers.SGD(learning_rate=0.001)

model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])
model.summary()

batch_size = 128
epochs = 50

h = model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_split=0.2)

plt.scatter(np.arange(epochs),h.history['loss'])
plt.scatter(np.arange(epochs),h.history['val_loss'],c='r')
plt.show()

plt.scatter(np.arange(epochs),h.history['accuracy'])
plt.scatter(np.arange(epochs),h.history['val_accuracy'],c='y')
plt.show()

score = model.evaluate(test_images, test_labels, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

def plot_image(img_index):
    label_index = test_labels[img_index]
    plt.imshow(test_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

picture = test_data[img_index].reshape(-1,784)

model.predict(picture)

"""#Opis:

batch_size = 128

epochs = 50

Zwiększony zbiór treningowy z **60000** do **68000** (20% to zbiór walidacyjny)

opt = keras.optimizers.Adam(learning_rate=0.001)
"""

model.summary()

"""Wnioski i komentarz

Model uczy się do około 4 epoki, potem praktycznie się nie uczy, wykres błędu (treningowego i walidacyjnego) jest praktycznie na stałym poziomie.

--------------------------------------------------------------------------------------------------
"""

(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.mnist.load_data()

"""One-hot encoding"""

train_labels = tf.keras.utils.to_categorical(train_labels, 10)
test_labels = tf.keras.utils.to_categorical(test_labels, 10)

train_data.shape,train_labels.shape

test_data.shape,test_labels.shape

train_labels[0]

"""# **Regularyzacja** - metoda 2

Zmniejszamy **wielkość modelu**:
"""



train_images = train_data.reshape((-1, 784))
test_images = test_data.reshape((-1, 784))

model = Sequential()
model.add(Dense(units = 64, use_bias=True, input_shape=(784,), activation = "relu"))
model.add(Dense(units = 10, use_bias=True, activation = "softmax"))

opt = keras.optimizers.Adam(learning_rate=0.001)
#opt = keras.optimizers.SGD(learning_rate=0.001)

model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])
model.summary()

batch_size = 128
epochs = 50

h = model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_split=0.2)

plt.scatter(np.arange(epochs),h.history['loss'])
plt.scatter(np.arange(epochs),h.history['val_loss'],c='r')
plt.show()

plt.scatter(np.arange(epochs),h.history['accuracy'])
plt.scatter(np.arange(epochs),h.history['val_accuracy'],c='y')
plt.show()

score = model.evaluate(test_images, test_labels, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

def plot_image(img_index):
    label_index = test_labels[img_index]
    plt.imshow(test_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

picture = test_data[img_index].reshape(-1,784)

model.predict(picture)



"""#Opis:

batch_size = 128

epochs = 50

Zbiór treningowy  **60000** (20% to zbiór walidacyjny)

opt = keras.optimizers.Adam(learning_rate=0.001)
"""

model.summary()

"""Wnioski i komentarz

Model uczy się, mniewięcej do 35 epoki, poźniej następuje lekkie niewielkie przeuczeniem wykresy błędu (treningowego i walidacyjnego) się obijają

# **Regularyzacja** - metoda 3

Import normy L2:
"""

from keras.regularizers import l2

"""Danie regularyzacji L2 do warstw:

Visulization
"""

def plot_image(img_index):
    label_index = test_labels[img_index]
    plt.imshow(test_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

train_images = train_data.reshape((-1, 784))
test_images = test_data.reshape((-1, 784))

model = Sequential()
model.add(Dense(units = 128,kernel_regularizer=l2(0.01), use_bias=True, input_shape=(784,), activation = "relu"))
model.add(Dense(units = 10,kernel_regularizer=l2(0.01), use_bias=True, activation = "softmax"))

opt = keras.optimizers.Adam(learning_rate=0.001)
#opt = keras.optimizers.SGD(learning_rate=0.001)

model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])
model.summary()

batch_size = 128
epochs = 50

h = model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_split=0.2)

plt.scatter(np.arange(epochs),h.history['loss'])
plt.scatter(np.arange(epochs),h.history['val_loss'],c='r')
plt.show()

plt.scatter(np.arange(epochs),h.history['accuracy'])
plt.scatter(np.arange(epochs),h.history['val_accuracy'],c='y')
plt.show()

score = model.evaluate(test_images, test_labels, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

def plot_image(img_index):
    label_index = test_labels[img_index]
    plt.imshow(test_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

picture = test_data[img_index].reshape(-1,784)

model.predict(picture)



"""#Opis:

batch_size = 128

epochs = 50

Zwiększony zbiór treningowy z **60000** do **68000** (20% to zbiór walidacyjny)

opt = keras.optimizers.Adam(learning_rate=0.001)

kernel_regularizer=l2(0.01) (we wszystkich warstwach)
"""

model.summary()

"""Wnioski i komentarz

Model uczy się do około 30 epoki potem się praktycznie nie uczy, ale się nie przeucza, wykresy błędu (treningowego i walidacyjnego)są podobne przy czym błędy cały czas spadają, ale od około 30 epoki spada bardzo wolno wręcz są stałe. Dokładność modelu dla danych treningowych i walidacyjnych praktycznie cały czas rośnie, ale po 30 epoce mniej.

# **Regularyzacja** - metoda 4
"""

from keras.layers import Dropout

"""Visulization"""

def plot_image(img_index):
    label_index = test_labels[img_index]
    plt.imshow(test_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

train_images = train_data.reshape((-1, 784))
test_images = test_data.reshape((-1, 784))

model = Sequential()
model.add(Dense(units = 128, use_bias=True, input_shape=(784,), activation = "relu"))
model.add(Dropout(0.4))
model.add(Dense(units = 10, use_bias=True, activation = "softmax"))

opt = keras.optimizers.Adam(learning_rate=0.001)
#opt = keras.optimizers.SGD(learning_rate=0.001)

model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])
model.summary()

batch_size = 128
epochs = 50

h = model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_split=0.2)

plt.scatter(np.arange(epochs),h.history['loss'])
plt.scatter(np.arange(epochs),h.history['val_loss'],c='r')
plt.show()

plt.scatter(np.arange(epochs),h.history['accuracy'])
plt.scatter(np.arange(epochs),h.history['val_accuracy'],c='y')
plt.show()

score = model.evaluate(test_images, test_labels, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

def plot_image(img_index):
    label_index = test_labels[img_index]
    plt.imshow(test_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

picture = test_data[img_index].reshape(-1,784)

model.predict(picture)

"""#Opis:

batch_size = 128

epochs = 50

Zbiór treningowy **60000** (20% to zbiór walidacyjny)

opt = keras.optimizers.Adam(learning_rate=0.001)

model.add(Dropout(0.4))
"""

model.summary()

"""Wnioski i komentarz

Model uczy się do samego końca i się nie przeucza, wykresy błędu (treningowego i walidacyjnego)są podobne przy czym błędy cały czas spadają. Dokładność modelu dla danych treningowych i walidacyjnych praktycznie cały czas rośnie.

## Regularyzacja all in one #1
"""





"""Zwiększamy zbiór treningowy z **60000** do **68000** (20% to zbiór walidacyjny)"""

test_data = data[:68000]
train_data = data[68000:]
test_labels = label[:68000]
train_labels = label[68000:]

print(test_data.shape,train_data.shape,test_labels.shape,train_labels.shape)

"""One-hot coding"""

train_labels = tf.keras.utils.to_categorical(train_labels, 10)
test_labels = tf.keras.utils.to_categorical(test_labels, 10)

train_data.shape,train_labels.shape

test_data.shape,test_labels.shape

train_labels[0]

def plot_image(img_index):
    label_index = train_labels[img_index]
    plt.imshow(train_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

train_images = train_data.reshape((-1, 784))
test_images = test_data.reshape((-1, 784))

"""Danie **regularyzacji L2** do warstw:

Adding dropout layer

Resizing model
"""

model = Sequential()
model.add(Dense(units = 64, kernel_regularizer=l2(0.01), use_bias=True, input_shape=(784,), activation = "relu"))
model.add(Dropout(0.4))
model.add(Dense(units = 10, kernel_regularizer=l2(0.01), use_bias=True, activation = "softmax"))

opt = keras.optimizers.Adam(learning_rate=0.001)
#opt = keras.optimizers.SGD(learning_rate=0.001)

model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])
model.summary()

batch_size = 128
epochs = 50

h = model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_split=0.2)

plt.scatter(np.arange(epochs),h.history['loss'])
plt.scatter(np.arange(epochs),h.history['val_loss'],c='r')
plt.show()

plt.scatter(np.arange(epochs),h.history['accuracy'])
plt.scatter(np.arange(epochs),h.history['val_accuracy'],c='y')
plt.show()

score = model.evaluate(test_images, test_labels, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

def plot_image(img_index):
    label_index = train_labels[img_index]
    plt.imshow(train_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

picture = train_data[img_index].reshape(-1,784)

model.predict(picture)



"""#Opis:

batch_size = 128

epochs = 50

Zbiór treningowy zwiększony z ***60000*** do **68000* (20% to zbiór walidacyjny)

opt = keras.optimizers.Adam(learning_rate=0.001)

kernel_regularizer=l2(0.01) we wszystkich warstwach

model.add(Dropout(0.4))
"""

model.summary()

"""Wnioski i komentarz

Model uczy się do samego końca i się nie przeucza, wykresy błędu (treningowego i walidacyjnego)są podobne przy czym błędy cały czas, ale od 5 epoki dużo wolniej. Dokładność modelu dla danych treningowych i walidacyjnych praktycznie cały czas rośnie.

## Regularyzacja all in one #2
"""



"""Zwiększamy zbiór treningowy z **60000** do **68000** (20% to zbiór walidacyjny)"""

test_data = data[:68000]
train_data = data[68000:]
test_labels = label[:68000]
train_labels = label[68000:]

print(test_data.shape,train_data.shape,test_labels.shape,train_labels.shape)

"""One-hot coding"""

train_labels = tf.keras.utils.to_categorical(train_labels, 10)
test_labels = tf.keras.utils.to_categorical(test_labels, 10)

train_data.shape,train_labels.shape

test_data.shape,test_labels.shape

train_labels[0]

def plot_image(img_index):
    label_index = train_labels[img_index]
    plt.imshow(train_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

train_images = train_data.reshape((-1, 784))
test_images = test_data.reshape((-1, 784))

"""Danie **regularyzacji L2** do warstw:

Adding dropout layer

Resizing model
"""

model = Sequential()
model.add(Dense(units = 64, kernel_regularizer=l2(0.01), use_bias=True, input_shape=(784,), activation = "relu"))
model.add(Dropout(0.4))
model.add(Dense(units = 10, kernel_regularizer=l2(0.01), use_bias=True, activation = "softmax"))

opt = keras.optimizers.Adam(learning_rate=0.001)
#opt = keras.optimizers.SGD(learning_rate=0.001)

model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])
model.summary()

batch_size = 128
epochs = 75

h = model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_split=0.2)

plt.scatter(np.arange(epochs),h.history['loss'])
plt.scatter(np.arange(epochs),h.history['val_loss'],c='r')
plt.show()

plt.scatter(np.arange(epochs),h.history['accuracy'])
plt.scatter(np.arange(epochs),h.history['val_accuracy'],c='y')
plt.show()

score = model.evaluate(test_images, test_labels, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

def plot_image(img_index):
    label_index = train_labels[img_index]
    plt.imshow(train_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

picture = train_data[img_index].reshape(-1,784)

model.predict(picture)



"""#Opis:

batch_size = 128

epochs = 75

Zbiór treningowy zwiększony z ***60000*** do **68000* (20% to zbiór walidacyjny)

opt = keras.optimizers.Adam(learning_rate=0.001)

kernel_regularizer=l2(0.01) we wszystkich warstwach

model.add(Dropout(0.4))
"""

model.summary()

"""Wnioski i komentarz

Model uczy się do około 30 epoki potem deliktanie się przeucza, wykresy błędu (treningowego i walidacyjnego) są podobne przy czym po 30 epoce błąd validacyjny rośnie dalej, a błąd treningowy spada. Dokładność modelu dla danych treningowych i walidacyjnych praktycznie cały czas rośnie.

## Regularyzacja all in one #3
"""



"""Zwiększamy zbiór treningowy z **60000** do **68000** (20% to zbiór walidacyjny)"""

test_data = data[:68000]
train_data = data[68000:]
test_labels = label[:68000]
train_labels = label[68000:]

print(test_data.shape,train_data.shape,test_labels.shape,train_labels.shape)

"""One-hot coding"""

train_labels = tf.keras.utils.to_categorical(train_labels, 10)
test_labels = tf.keras.utils.to_categorical(test_labels, 10)

train_data.shape,train_labels.shape

test_data.shape,test_labels.shape

train_labels[0]

def plot_image(img_index):
    label_index = train_labels[img_index]
    plt.imshow(train_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

train_images = train_data.reshape((-1, 784))
test_images = test_data.reshape((-1, 784))

"""Danie **regularyzacji L2** do warstw:

Adding dropout layer

Resizing model
"""

model = Sequential()
model.add(Dense(units = 64, kernel_regularizer=l2(0.01), use_bias=True, input_shape=(784,), activation = "relu"))
model.add(Dropout(0.3))
model.add(Dense(units = 10, kernel_regularizer=l2(0.01), use_bias=True, activation = "softmax"))

opt = keras.optimizers.Adam(learning_rate=0.002)
#opt = keras.optimizers.SGD(learning_rate=0.001)

model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])
model.summary()

batch_size = 128
epochs = 100

h = model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_split=0.2)

plt.scatter(np.arange(epochs),h.history['loss'])
plt.scatter(np.arange(epochs),h.history['val_loss'],c='r')
plt.show()

plt.scatter(np.arange(epochs),h.history['accuracy'])
plt.scatter(np.arange(epochs),h.history['val_accuracy'],c='y')
plt.show()

score = model.evaluate(test_images, test_labels, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

def plot_image(img_index):
    label_index = train_labels[img_index]
    plt.imshow(train_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

picture = train_data[img_index].reshape(-1,784)

model.predict(picture)



"""#Opis:

batch_size = 128

epochs = 100

Zbiór treningowy zwiększony z ***60000*** do **68000* (20% to zbiór walidacyjny)

opt = keras.optimizers.Adam(learning_rate=0.002)

kernel_regularizer=l2(0.01) we wszystkich warstwach

model.add(Dropout(0.3))
"""

model.summary()

"""Wnioski i komentarz

Model uczy się do około 4 epoki potem się przeucza, wykresy błędu (treningowego i walidacyjnego) są podobne przy czym po 5 epoce błąd validacyjny rośnie dalej, a błąd treningowy spada. Dokładność modelu dla danych treningowych i walidacyjnych praktycznie cały czas rośnie.

## Regularyzacja all in one #4

Zwiększamy zbiór treningowy z **60000** do **68000** (20% to zbiór walidacyjny)
"""

test_data = data[:68000]
train_data = data[68000:]
test_labels = label[:68000]
train_labels = label[68000:]

print(test_data.shape,train_data.shape,test_labels.shape,train_labels.shape)

"""One-hot coding"""

train_labels = tf.keras.utils.to_categorical(train_labels, 10)
test_labels = tf.keras.utils.to_categorical(test_labels, 10)

train_data.shape,train_labels.shape

test_data.shape,test_labels.shape

train_labels[0]

def plot_image(img_index):
    label_index = train_labels[img_index]
    plt.imshow(train_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

train_images = train_data.reshape((-1, 784))
test_images = test_data.reshape((-1, 784))

"""Danie **regularyzacji L2** do warstw:

Adding dropout layer

Zmiana learning_rate

Resizing model
"""

model = Sequential()
model.add(Dense(units = 128, kernel_regularizer=l2(0.01), use_bias=True, input_shape=(784,), activation = "relu"))
model.add(Dropout(0.3))
model.add(Dense(units = 64, kernel_regularizer=l2(0.01), use_bias=True,  activation = "relu"))
model.add(Dropout(0.3))
model.add(Dense(units = 10, kernel_regularizer=l2(0.01), use_bias=True, activation = "softmax"))

opt = keras.optimizers.Adam(learning_rate=0.009)
#opt = keras.optimizers.SGD(learning_rate=0.001)

model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])
model.summary()

batch_size = 1024
epochs = 150

h = model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_split=0.2)

plt.scatter(np.arange(epochs),h.history['loss'])
plt.scatter(np.arange(epochs),h.history['val_loss'],c='r')
plt.show()

plt.scatter(np.arange(epochs),h.history['accuracy'])
plt.scatter(np.arange(epochs),h.history['val_accuracy'],c='y')
plt.show()

score = model.evaluate(test_images, test_labels, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])

def plot_image(img_index):
    label_index = train_labels[img_index]
    plt.imshow(train_data[img_index]/255, cmap = 'gray')
    print(label_index)

img_index = 10
plot_image(img_index)

picture = train_data[img_index].reshape(-1,784)

model.predict(picture)



"""#Opis:

batch_size = 1024

epochs = 150

Zbiór treningowy zwiększony z ***60000*** do **68000* (20% to zbiór walidacyjny)

opt = keras.optimizers.Adam(learning_rate=0.009)

kernel_regularizer=l2(0.01) we wszystkich warstwach

model.add(Dropout(0.4))
"""

model.summary()

"""Wnioski i komentarz

Model uczy się cały czas, przy czym po 20 epoce dużo wolniej, wykresy błędu (treningowego i walidacyjnego) są podobne przy czym po 20 epoce wykresy spadają wolniej. Dokładność modelu dla danych treningowych i walidacyjnych cały czas rośnie.
"""

